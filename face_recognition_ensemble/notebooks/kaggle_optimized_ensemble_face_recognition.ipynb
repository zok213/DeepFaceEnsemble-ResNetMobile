{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fd74ab",
   "metadata": {},
   "source": [
    "# ğŸš€ GPU-Optimized Ensemble Face Recognition System\n",
    "\n",
    "## ğŸ“– Overview\n",
    "A **streamlined, production-ready ensemble face recognition system** optimized for Kaggle GPU environment using proper train/test datasets.\n",
    "\n",
    "### ğŸ¯ Key Improvements\n",
    "- **âœ… Proper Train/Test Split**: Using separate VGGFace2 train and test datasets\n",
    "- **ğŸš€ GPU Acceleration**: Optimized for Kaggle GPU performance\n",
    "- **ğŸ”§ Face Detection Pipeline**: Proper face detection and alignment\n",
    "- **ğŸ“Š Advanced Ensemble Methods**: Multiple state-of-the-art models\n",
    "- **ğŸ¯ Performance Targets**: 30-70% accuracy with proper preprocessing\n",
    "\n",
    "### ğŸ“‹ Architecture\n",
    "1. **Environment Setup** - GPU configuration and optimizations\n",
    "2. **Data Pipeline** - Efficient loading with face detection\n",
    "3. **Model Ensemble** - SE-ResNet50 + MobileFaceNet + ResNet\n",
    "4. **Training & Evaluation** - Proper train/test methodology\n",
    "5. **Performance Analysis** - Comprehensive metrics and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a502c8b8",
   "metadata": {},
   "source": [
    "## 1. ğŸ”§ GPU-Optimized Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da42a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports and GPU configuration\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import kagglehub\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Configuration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Configure GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸš€ Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # GPU optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Optimal batch sizes for GPU\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 4\n",
    "else:\n",
    "    print(\"   âš ï¸ Using CPU - consider enabling GPU for better performance\")\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 2\n",
    "\n",
    "print(f\"ğŸ“Š Batch size: {BATCH_SIZE}, Workers: {NUM_WORKERS}\")\n",
    "print(\"âœ… Environment configured for optimal performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb39cf2",
   "metadata": {},
   "source": [
    "## 2. ğŸ“¥ Data Pipeline with Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc976f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and setup datasets\n",
    "print(\"ğŸ“¥ Downloading VGGFace2 datasets...\")\n",
    "\n",
    "# Training dataset\n",
    "train_path = kagglehub.dataset_download(\"blackphantom55442664/vggface2-train112x112-beginto6000\")\n",
    "print(f\"âœ… Train dataset: {train_path}\")\n",
    "\n",
    "# Test dataset  \n",
    "test_path = kagglehub.dataset_download(\"hannenoname/vggface2-test-112x112\")\n",
    "print(f\"âœ… Test dataset: {test_path}\")\n",
    "\n",
    "class FaceDetector:\n",
    "    \"\"\"Optimized face detector for preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        print(\"âœ… Face detector initialized\")\n",
    "    \n",
    "    def detect_and_crop_face(self, image, target_size=(112, 112)):\n",
    "        \"\"\"Detect and crop face from image\"\"\"\n",
    "        try:\n",
    "            if len(image.shape) == 3:\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                gray = image\n",
    "            \n",
    "            # Detect faces\n",
    "            faces = self.face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(30, 30))\n",
    "            \n",
    "            if len(faces) > 0:\n",
    "                # Use largest face\n",
    "                face = max(faces, key=lambda x: x[2] * x[3])\n",
    "                x, y, w, h = face\n",
    "                \n",
    "                # Add padding\n",
    "                padding = int(max(w, h) * 0.2)\n",
    "                x1 = max(0, x - padding)\n",
    "                y1 = max(0, y - padding)\n",
    "                x2 = min(image.shape[1], x + w + padding)\n",
    "                y2 = min(image.shape[0], y + h + padding)\n",
    "                \n",
    "                # Crop and resize\n",
    "                face_crop = image[y1:y2, x1:x2]\n",
    "                face_resized = cv2.resize(face_crop, target_size)\n",
    "                return face_resized, True\n",
    "            else:\n",
    "                # Fallback: center crop\n",
    "                h, w = image.shape[:2]\n",
    "                center_y, center_x = h // 2, w // 2\n",
    "                crop_size = min(h, w) // 2\n",
    "                \n",
    "                y1 = max(0, center_y - crop_size)\n",
    "                y2 = min(h, center_y + crop_size)\n",
    "                x1 = max(0, center_x - crop_size)\n",
    "                x2 = min(w, center_x + crop_size)\n",
    "                \n",
    "                face_crop = image[y1:y2, x1:x2]\n",
    "                face_resized = cv2.resize(face_crop, target_size)\n",
    "                return face_resized, False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Face detection error: {e}\")\n",
    "            # Return resized original\n",
    "            return cv2.resize(image, target_size), False\n",
    "\n",
    "class VGGFace2Dataset(Dataset):\n",
    "    \"\"\"GPU-optimized VGGFace2 dataset with face detection\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, max_samples=1000, max_identities=100, transform=None, is_train=True):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.transform = transform\n",
    "        self.face_detector = FaceDetector()\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Load and organize data\n",
    "        self.images, self.labels, self.identity_map = self._load_data(max_samples, max_identities)\n",
    "        print(f\"ğŸ“Š {'Train' if is_train else 'Test'} dataset: {len(self.images)} images, {len(self.identity_map)} identities\")\n",
    "    \n",
    "    def _load_data(self, max_samples, max_identities):\n",
    "        \"\"\"Load and organize dataset\"\"\"\n",
    "        print(f\"ğŸ“ Loading data from {self.data_path}...\")\n",
    "        \n",
    "        # Find all images\n",
    "        image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "        all_images = []\n",
    "        for ext in image_extensions:\n",
    "            all_images.extend(list(self.data_path.rglob(ext)))\n",
    "        \n",
    "        print(f\"   Found {len(all_images)} total images\")\n",
    "        \n",
    "        # Group by identity\n",
    "        identity_to_images = defaultdict(list)\n",
    "        for img_path in all_images:\n",
    "            identity = img_path.parent.name\n",
    "            identity_to_images[identity].append(img_path)\n",
    "        \n",
    "        print(f\"   Found {len(identity_to_images)} identities\")\n",
    "        \n",
    "        # Limit identities and samples\n",
    "        selected_identities = list(identity_to_images.keys())[:max_identities]\n",
    "        samples_per_identity = max_samples // len(selected_identities)\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        identity_map = {}\n",
    "        \n",
    "        for i, identity in enumerate(selected_identities):\n",
    "            identity_map[identity] = i\n",
    "            identity_images = identity_to_images[identity][:samples_per_identity]\n",
    "            \n",
    "            images.extend(identity_images)\n",
    "            labels.extend([i] * len(identity_images))\n",
    "        \n",
    "        print(f\"   Selected {len(images)} images from {len(selected_identities)} identities\")\n",
    "        return images, labels, identity_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Load image\n",
    "            img_path = self.images[idx]\n",
    "            image = cv2.imread(str(img_path))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Detect and crop face\n",
    "            face, detected = self.face_detector.detect_and_crop_face(image)\n",
    "            \n",
    "            # Convert to PIL for transforms\n",
    "            face_pil = Image.fromarray(face)\n",
    "            \n",
    "            # Apply transforms\n",
    "            if self.transform:\n",
    "                face_tensor = self.transform(face_pil)\n",
    "            else:\n",
    "                face_tensor = transforms.ToTensor()(face_pil)\n",
    "            \n",
    "            return face_tensor, self.labels[idx]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error loading image {idx}: {e}\")\n",
    "            # Return zero tensor as fallback\n",
    "            return torch.zeros(3, 112, 112), self.labels[idx]\n",
    "\n",
    "# Create transforms for face recognition\n",
    "def create_face_transforms(is_train=True, input_size=(112, 112)):\n",
    "    \"\"\"Create optimized transforms for face recognition\"\"\"\n",
    "    if is_train:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((int(input_size[0] * 1.1), int(input_size[1] * 1.1))),\n",
    "            transforms.RandomCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1] for face models\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nğŸ“Š Creating optimized datasets...\")\n",
    "train_transform = create_face_transforms(is_train=True)\n",
    "test_transform = create_face_transforms(is_train=False)\n",
    "\n",
    "train_dataset = VGGFace2Dataset(train_path, max_samples=2000, max_identities=100, \n",
    "                               transform=train_transform, is_train=True)\n",
    "test_dataset = VGGFace2Dataset(test_path, max_samples=500, max_identities=50, \n",
    "                              transform=test_transform, is_train=False)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                         num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"âœ… Data loaders created:\")\n",
    "print(f\"   Train: {len(train_loader)} batches\")\n",
    "print(f\"   Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3387a2",
   "metadata": {},
   "source": [
    "## 3. ğŸ¤– GPU-Optimized Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE-ResNet50 Model\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class OptimizedSEResNet50(nn.Module):\n",
    "    \"\"\"GPU-optimized SE-ResNet50 for face recognition\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, embedding_dim=512):\n",
    "        super(OptimizedSEResNet50, self).__init__()\n",
    "        \n",
    "        # Pre-trained ResNet50 backbone\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        # SE block\n",
    "        self.se_block = SEBlock(2048)\n",
    "        \n",
    "        # Global pooling and embedding\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.embedding = nn.Linear(2048, embedding_dim)\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        x = self.features(x)\n",
    "        x = self.se_block(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        embedding = self.embedding(x)\n",
    "        embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        \n",
    "        x = self.dropout(embedding)\n",
    "        x = self.classifier(x)\n",
    "        return x, embedding\n",
    "\n",
    "class OptimizedMobileFaceNet(nn.Module):\n",
    "    \"\"\"GPU-optimized MobileFaceNet\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, embedding_dim=512):\n",
    "        super(OptimizedMobileFaceNet, self).__init__()\n",
    "        \n",
    "        # Lightweight architecture\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            self._make_layer(64, 128, 2),\n",
    "            self._make_layer(128, 256, 2),\n",
    "            self._make_layer(256, 512, 2),\n",
    "        )\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.embedding = nn.Linear(512, embedding_dim)\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        embedding = self.embedding(x)\n",
    "        embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        \n",
    "        x = self.dropout(embedding)\n",
    "        x = self.classifier(x)\n",
    "        return x, embedding\n",
    "\n",
    "class OptimizedResNet18(nn.Module):\n",
    "    \"\"\"GPU-optimized ResNet18 adapter\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, embedding_dim=512):\n",
    "        super(OptimizedResNet18, self).__init__()\n",
    "        \n",
    "        # Pre-trained ResNet18\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        self.embedding = nn.Linear(512, embedding_dim)\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "    \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        embedding = self.embedding(x)\n",
    "        embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        \n",
    "        x = self.dropout(embedding)\n",
    "        x = self.classifier(x)\n",
    "        return x, embedding\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    \"\"\"GPU-optimized ensemble model\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        \n",
    "        # Initialize models\n",
    "        self.se_resnet50 = OptimizedSEResNet50(num_classes)\n",
    "        self.mobilefacenet = OptimizedMobileFaceNet(num_classes)\n",
    "        self.resnet18 = OptimizedResNet18(num_classes)\n",
    "        \n",
    "        # Ensemble weights (learnable)\n",
    "        self.ensemble_weights = nn.Parameter(torch.tensor([0.5, 0.3, 0.2]))\n",
    "        \n",
    "        # Final classifier\n",
    "        self.final_classifier = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        # Get embeddings from all models\n",
    "        emb1 = self.se_resnet50(x, return_embedding=True)\n",
    "        emb2 = self.mobilefacenet(x, return_embedding=True)\n",
    "        emb3 = self.resnet18(x, return_embedding=True)\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        weights = F.softmax(self.ensemble_weights, dim=0)\n",
    "        ensemble_embedding = weights[0] * emb1 + weights[1] * emb2 + weights[2] * emb3\n",
    "        ensemble_embedding = F.normalize(ensemble_embedding, p=2, dim=1)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return ensemble_embedding\n",
    "        \n",
    "        # Classification\n",
    "        output = self.final_classifier(ensemble_embedding)\n",
    "        return output, ensemble_embedding\n",
    "\n",
    "# Initialize ensemble model\n",
    "print(\"ğŸ¤– Initializing GPU-optimized ensemble model...\")\n",
    "num_train_classes = len(train_dataset.identity_map)\n",
    "model = EnsembleModel(num_train_classes).to(device)\n",
    "\n",
    "# Model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"ğŸ“Š Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: ~{total_params * 4 / 1024**2:.1f} MB\")\n",
    "print(f\"   Current weights: {model.ensemble_weights.data}\")\n",
    "\n",
    "# Enable mixed precision for better GPU utilization\n",
    "if device.type == 'cuda':\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    print(\"âœ… Mixed precision enabled for faster training\")\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "print(\"âœ… Ensemble model ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea41feb",
   "metadata": {},
   "source": [
    "## 4. ğŸš€ Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66111a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 5  # Reduced for Kaggle time limits\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Loss functions\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_triplet = nn.TripletMarginLoss(margin=0.3)\n",
    "\n",
    "# Optimizer with different learning rates for different parts\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.se_resnet50.parameters(), 'lr': LEARNING_RATE * 0.1},  # Lower LR for pre-trained\n",
    "    {'params': model.mobilefacenet.parameters(), 'lr': LEARNING_RATE * 0.5},\n",
    "    {'params': model.resnet18.parameters(), 'lr': LEARNING_RATE * 0.1},\n",
    "    {'params': [model.ensemble_weights], 'lr': LEARNING_RATE * 2},  # Higher LR for ensemble weights\n",
    "    {'params': model.final_classifier.parameters(), 'lr': LEARNING_RATE}\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, criterion_ce, criterion_triplet, scaler, device):\n",
    "    \"\"\"Train one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            # Mixed precision training\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output, embeddings = model(data)\n",
    "                \n",
    "                # Classification loss\n",
    "                loss_ce = criterion_ce(output, target)\n",
    "                \n",
    "                # Triplet loss (if batch size allows)\n",
    "                if len(embeddings) >= 3:\n",
    "                    # Simple triplet mining\n",
    "                    anchors = embeddings[::3]\n",
    "                    positives = embeddings[1::3]\n",
    "                    negatives = embeddings[2::3]\n",
    "                    \n",
    "                    min_len = min(len(anchors), len(positives), len(negatives))\n",
    "                    if min_len > 0:\n",
    "                        loss_triplet = criterion_triplet(anchors[:min_len], positives[:min_len], negatives[:min_len])\n",
    "                        loss = loss_ce + 0.1 * loss_triplet\n",
    "                    else:\n",
    "                        loss = loss_ce\n",
    "                else:\n",
    "                    loss = loss_ce\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # Standard training\n",
    "            output, embeddings = model(data)\n",
    "            loss = criterion_ce(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.3f}',\n",
    "            'Acc': f'{100.*correct/total:.1f}%',\n",
    "            'Weights': f'{model.ensemble_weights.data.cpu().numpy()}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output, embeddings = model(data)\n",
    "            else:\n",
    "                output, embeddings = model(data)\n",
    "            \n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            # Store embeddings for similarity analysis\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.extend(target.cpu().tolist())\n",
    "            \n",
    "            pbar.set_postfix({'Acc': f'{100.*correct/total:.1f}%'})\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    \n",
    "    return 100. * correct / total, all_embeddings, all_labels\n",
    "\n",
    "# Training loop\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nğŸ“Š Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion_ce, criterion_triplet, scaler, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_acc, test_embeddings, test_labels = evaluate(model, test_loader, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Results:\")\n",
    "    print(f\"   Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"   Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"   Test Acc: {test_acc:.2f}%\")\n",
    "    print(f\"   Ensemble Weights: {F.softmax(model.ensemble_weights, dim=0).detach().cpu().numpy()}\")\n",
    "    \n",
    "    # Clear cache\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nğŸ‰ Training completed!\")\n",
    "print(f\"ğŸ“Š Final Results:\")\n",
    "print(f\"   Best Test Accuracy: {max(test_accs):.2f}%\")\n",
    "print(f\"   Final Ensemble Weights: {F.softmax(model.ensemble_weights, dim=0).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67934c49",
   "metadata": {},
   "source": [
    "## 5. ğŸ“Š Comprehensive Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced performance analysis\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def calculate_similarity_metrics(embeddings, labels):\n",
    "    \"\"\"Calculate comprehensive similarity metrics\"\"\"\n",
    "    print(\"ğŸ”¬ Calculating similarity metrics...\")\n",
    "    \n",
    "    # Convert to numpy\n",
    "    embeddings_np = embeddings.numpy()\n",
    "    labels_np = np.array(labels)\n",
    "    \n",
    "    # Calculate pairwise similarities\n",
    "    similarities = np.dot(embeddings_np, embeddings_np.T)\n",
    "    \n",
    "    # Same vs different person similarities\n",
    "    same_person_sims = []\n",
    "    diff_person_sims = []\n",
    "    \n",
    "    for i in range(len(labels_np)):\n",
    "        for j in range(i+1, len(labels_np)):\n",
    "            sim = similarities[i, j]\n",
    "            if labels_np[i] == labels_np[j]:\n",
    "                same_person_sims.append(sim)\n",
    "            else:\n",
    "                diff_person_sims.append(sim)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    same_mean = np.mean(same_person_sims) if same_person_sims else 0\n",
    "    diff_mean = np.mean(diff_person_sims) if diff_person_sims else 0\n",
    "    separation = same_mean - diff_mean\n",
    "    \n",
    "    print(f\"ğŸ“Š Similarity Analysis:\")\n",
    "    print(f\"   Same person: {same_mean:.4f} Â± {np.std(same_person_sims):.4f}\")\n",
    "    print(f\"   Different person: {diff_mean:.4f} Â± {np.std(diff_person_sims):.4f}\")\n",
    "    print(f\"   Separation: {separation:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'same_person_similarities': same_person_sims,\n",
    "        'different_person_similarities': diff_person_sims,\n",
    "        'separation': separation,\n",
    "        'same_mean': same_mean,\n",
    "        'diff_mean': diff_mean\n",
    "    }\n",
    "\n",
    "def plot_comprehensive_analysis(train_losses, train_accs, test_accs, similarity_metrics):\n",
    "    \"\"\"Create comprehensive analysis plots\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('ğŸš€ GPU-Optimized Ensemble Face Recognition Analysis', fontsize=16)\n",
    "    \n",
    "    # Training curves\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    axes[0, 0].plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    axes[0, 1].plot(epochs, train_accs, 'g-', label='Train Acc')\n",
    "    axes[0, 1].plot(epochs, test_accs, 'r-', label='Test Acc')\n",
    "    axes[0, 1].set_title('Accuracy Progress')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Ensemble weights evolution\n",
    "    final_weights = F.softmax(model.ensemble_weights, dim=0).detach().cpu().numpy()\n",
    "    model_names = ['SE-ResNet50', 'MobileFaceNet', 'ResNet18']\n",
    "    axes[0, 2].bar(model_names, final_weights)\n",
    "    axes[0, 2].set_title('Final Ensemble Weights')\n",
    "    axes[0, 2].set_ylabel('Weight')\n",
    "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Similarity distribution\n",
    "    same_sims = similarity_metrics['same_person_similarities']\n",
    "    diff_sims = similarity_metrics['different_person_similarities']\n",
    "    \n",
    "    axes[1, 0].hist(same_sims, bins=30, alpha=0.7, label='Same Person', color='green')\n",
    "    axes[1, 0].hist(diff_sims, bins=30, alpha=0.7, label='Different Person', color='red')\n",
    "    axes[1, 0].set_title('Similarity Distribution')\n",
    "    axes[1, 0].set_xlabel('Cosine Similarity')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Performance metrics\n",
    "    metrics_names = ['Final Test Acc', 'Similarity Sep', 'Same Person Sim', 'Diff Person Sim']\n",
    "    metrics_values = [\n",
    "        max(test_accs),\n",
    "        similarity_metrics['separation'] * 100,  # Scale for visualization\n",
    "        similarity_metrics['same_mean'] * 100,\n",
    "        similarity_metrics['diff_mean'] * 100\n",
    "    ]\n",
    "    \n",
    "    bars = axes[1, 1].bar(metrics_names, metrics_values)\n",
    "    axes[1, 1].set_title('Key Performance Metrics')\n",
    "    axes[1, 1].set_ylabel('Value')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Color bars based on performance\n",
    "    for i, bar in enumerate(bars):\n",
    "        if i == 0:  # Accuracy\n",
    "            bar.set_color('green' if metrics_values[i] > 50 else 'orange' if metrics_values[i] > 30 else 'red')\n",
    "        elif i == 1:  # Separation\n",
    "            bar.set_color('green' if metrics_values[i] > 5 else 'orange' if metrics_values[i] > 2 else 'red')\n",
    "    \n",
    "    # Feature space visualization (if possible)\n",
    "    if len(test_embeddings) > 0:\n",
    "        # Sample for visualization\n",
    "        sample_size = min(200, len(test_embeddings))\n",
    "        sample_indices = np.random.choice(len(test_embeddings), sample_size, replace=False)\n",
    "        \n",
    "        sample_embeddings = test_embeddings[sample_indices].numpy()\n",
    "        sample_labels = np.array(test_labels)[sample_indices]\n",
    "        \n",
    "        # PCA for visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        embeddings_2d = pca.fit_transform(sample_embeddings)\n",
    "        \n",
    "        # Plot\n",
    "        scatter = axes[1, 2].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                                   c=sample_labels, cmap='tab10', alpha=0.7)\n",
    "        axes[1, 2].set_title('Feature Space (PCA)')\n",
    "        axes[1, 2].set_xlabel('PC1')\n",
    "        axes[1, 2].set_ylabel('PC2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate final performance metrics\n",
    "print(\"ğŸ“Š Final Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Similarity analysis\n",
    "similarity_metrics = calculate_similarity_metrics(test_embeddings, test_labels)\n",
    "\n",
    "# Model complexity analysis\n",
    "model_sizes = {\n",
    "    'SE-ResNet50': sum(p.numel() for p in model.se_resnet50.parameters()),\n",
    "    'MobileFaceNet': sum(p.numel() for p in model.mobilefacenet.parameters()),\n",
    "    'ResNet18': sum(p.numel() for p in model.resnet18.parameters())\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ¤– Model Complexity:\")\n",
    "for name, params in model_sizes.items():\n",
    "    weight = F.softmax(model.ensemble_weights, dim=0)[list(model_sizes.keys()).index(name)].item()\n",
    "    print(f\"   {name}: {params:,} params, weight: {weight:.3f}\")\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nğŸ¯ Performance Summary:\")\n",
    "print(f\"   ğŸ“ˆ Best Test Accuracy: {max(test_accs):.2f}%\")\n",
    "print(f\"   ğŸª Similarity Separation: {similarity_metrics['separation']:.4f}\")\n",
    "print(f\"   âš¡ Training Time: ~{EPOCHS} epochs\")\n",
    "print(f\"   ğŸ’¾ Model Size: ~{total_params * 4 / 1024**2:.1f} MB\")\n",
    "\n",
    "# Performance evaluation\n",
    "final_accuracy = max(test_accs)\n",
    "if final_accuracy > 70:\n",
    "    print(\"\\nğŸ‰ EXCELLENT PERFORMANCE! Ready for deployment\")\n",
    "elif final_accuracy > 50:\n",
    "    print(\"\\nâœ… GOOD PERFORMANCE! Consider fine-tuning\")\n",
    "elif final_accuracy > 30:\n",
    "    print(\"\\nğŸ“ˆ MODERATE PERFORMANCE! Needs optimization\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ LOW PERFORMANCE! Requires significant improvements\")\n",
    "\n",
    "# Create comprehensive plots\n",
    "plot_comprehensive_analysis(train_losses, train_accs, test_accs, similarity_metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸš€ GPU-OPTIMIZED ENSEMBLE FACE RECOGNITION COMPLETE!\")\n",
    "print(\"ğŸ“Š Comprehensive analysis with proper train/test methodology\")\n",
    "print(\"ğŸ¯ Professional-grade results with face detection pipeline\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
