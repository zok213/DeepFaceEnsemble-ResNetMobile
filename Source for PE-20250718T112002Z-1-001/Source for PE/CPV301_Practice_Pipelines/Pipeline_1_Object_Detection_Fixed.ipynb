{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd476db6",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Pipeline 1: Object Detection and Segmentation\n",
    "## Problem-Solving Pipeline for Classical Computer Vision\n",
    "\n",
    "### ðŸŽ¯ Problem Statement:\n",
    "**\"Given an image with objects, detect and segment them using classical computer vision techniques\"**\n",
    "\n",
    "### ðŸ“‹ Solution Approach:\n",
    "1. **Preprocessing** â†’ Noise reduction and enhancement\n",
    "2. **Thresholding** â†’ Separate objects from background\n",
    "3. **Morphological Operations** â†’ Clean up binary masks\n",
    "4. **Contour Detection** â†’ Find object boundaries\n",
    "5. **Analysis** â†’ Extract object properties\n",
    "\n",
    "### ðŸ”§ Techniques Used:\n",
    "- Image enhancement (histogram equalization, contrast stretching)\n",
    "- Smoothing filters (Gaussian, median)\n",
    "- Adaptive thresholding\n",
    "- Morphological operations (opening, closing)\n",
    "- Contour detection and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_results(images, titles, rows=2, cols=3, figsize=(15, 10)):\n",
    "    \"\"\"Display multiple images in a grid\"\"\"\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten() if rows * cols > 1 else [axes]\n",
    "    \n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if len(img.shape) == 3:\n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_test_objects():\n",
    "    \"\"\"Create a test image with objects for segmentation\"\"\"\n",
    "    img = np.zeros((400, 400, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Add various objects\n",
    "    cv2.rectangle(img, (50, 50), (150, 150), (255, 100, 100), -1)  # Red rectangle\n",
    "    cv2.circle(img, (300, 100), 50, (100, 255, 100), -1)          # Green circle\n",
    "    cv2.ellipse(img, (200, 250), (80, 40), 45, 0, 360, (100, 100, 255), -1)  # Blue ellipse\n",
    "    cv2.rectangle(img, (50, 300), (120, 370), (255, 255, 100), -1)  # Yellow rectangle\n",
    "    \n",
    "    # Add some noise\n",
    "    noise = np.random.randint(0, 30, img.shape, dtype=np.uint8)\n",
    "    img = cv2.add(img, noise)\n",
    "    \n",
    "    return img\n",
    "\n",
    "print(\"ðŸŽ¯ Object Detection and Segmentation Pipeline Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d09efe3",
   "metadata": {},
   "source": [
    "## ðŸ“– Step 1: Load and Analyze Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create test image\n",
    "try:\n",
    "    img = cv2.imread('D:/FPT_Material/Sem 4/CPV301/Source for PE/Image/image.jpg')\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(\"Image not found\")\n",
    "    print(\"âœ… Loaded image from file\")\n",
    "except:\n",
    "    print(\"âš ï¸ Creating test image with objects...\")\n",
    "    img = create_test_objects()\n",
    "\n",
    "# Convert to different formats\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "print(f\"ðŸ“Š Image Analysis:\")\n",
    "print(f\"Shape: {img.shape}\")\n",
    "print(f\"Data type: {img.dtype}\")\n",
    "print(f\"Brightness range: [{gray_img.min()}, {gray_img.max()}]\")\n",
    "print(f\"Mean brightness: {gray_img.mean():.1f}\")\n",
    "\n",
    "show_results(\n",
    "    [rgb_img, gray_img, hsv_img[:,:,0]],\n",
    "    ['Original RGB', 'Grayscale', 'HSV Hue Channel'],\n",
    "    rows=1, cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb478e5c",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 2: Preprocessing - Enhancement and Noise Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35129af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2.1 Histogram Equalization for contrast enhancement\n",
    "# ========================================\n",
    "equalized = cv2.equalizeHist(gray_img)\n",
    "\n",
    "# ========================================\n",
    "# 2.2 Contrast Stretching\n",
    "# ========================================\n",
    "stretched = cv2.normalize(gray_img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "# ========================================\n",
    "# 2.3 Noise Reduction\n",
    "# ========================================\n",
    "# Gaussian blur for general noise\n",
    "gaussian_blur = cv2.GaussianBlur(gray_img, (5, 5), 0)\n",
    "\n",
    "# Median filter for salt-and-pepper noise\n",
    "median_blur = cv2.medianBlur(gray_img, 5)\n",
    "\n",
    "# Bilateral filter for edge-preserving smoothing\n",
    "bilateral = cv2.bilateralFilter(gray_img, 9, 75, 75)\n",
    "\n",
    "print(\"ðŸ”§ Preprocessing Results:\")\n",
    "print(f\"Original contrast (std): {gray_img.std():.1f}\")\n",
    "print(f\"Equalized contrast (std): {equalized.std():.1f}\")\n",
    "print(f\"Stretched contrast (std): {stretched.std():.1f}\")\n",
    "\n",
    "show_results(\n",
    "    [gray_img, equalized, stretched, gaussian_blur, median_blur, bilateral],\n",
    "    ['Original', 'Equalized', 'Stretched', 'Gaussian Blur', 'Median Blur', 'Bilateral'],\n",
    "    rows=2, cols=3\n",
    ")\n",
    "\n",
    "# Choose the best preprocessed image for further processing\n",
    "# For object detection, bilateral filter often works best\n",
    "processed_img = bilateral\n",
    "print(\"âœ… Selected bilateral filtered image for further processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d799b",
   "metadata": {},
   "source": [
    "## ðŸ”’ Step 3: Thresholding - Separate Objects from Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b7b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3.1 Simple Thresholding\n",
    "# ========================================\n",
    "ret1, thresh_binary = cv2.threshold(processed_img, 127, 255, cv2.THRESH_BINARY)\n",
    "ret2, thresh_binary_inv = cv2.threshold(processed_img, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# ========================================\n",
    "# 3.2 Otsu's Automatic Thresholding\n",
    "# ========================================\n",
    "ret_otsu, otsu_thresh = cv2.threshold(processed_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "ret_otsu_inv, otsu_thresh_inv = cv2.threshold(processed_img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "# ========================================\n",
    "# 3.3 Adaptive Thresholding\n",
    "# ========================================\n",
    "adaptive_mean = cv2.adaptiveThreshold(processed_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "                                     cv2.THRESH_BINARY, 11, 2)\n",
    "adaptive_gaussian = cv2.adaptiveThreshold(processed_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                         cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "print(f\"ðŸ”’ Thresholding Analysis:\")\n",
    "print(f\"Simple threshold value: {ret1}\")\n",
    "print(f\"Otsu threshold value: {ret_otsu:.1f}\")\n",
    "print(f\"Otsu inverted threshold: {ret_otsu_inv:.1f}\")\n",
    "\n",
    "show_results(\n",
    "    [processed_img, thresh_binary, thresh_binary_inv, otsu_thresh, otsu_thresh_inv, adaptive_mean],\n",
    "    ['Preprocessed', 'Binary (127)', 'Binary Inv (127)', 'Otsu Binary', 'Otsu Inv', 'Adaptive Mean'],\n",
    "    rows=2, cols=3\n",
    ")\n",
    "\n",
    "# Choose the best thresholding result\n",
    "# For object detection, usually Otsu or adaptive works best\n",
    "# Let's analyze which gives better object separation\n",
    "def count_objects(binary_img):\n",
    "    \"\"\"Count potential objects by finding contours\"\"\"\n",
    "    contours, _ = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Filter out very small contours (noise)\n",
    "    significant_contours = [c for c in contours if cv2.contourArea(c) > 100]\n",
    "    return len(significant_contours)\n",
    "\n",
    "print(f\"\\nðŸ“Š Object Count Analysis:\")\n",
    "print(f\"Binary (127): {count_objects(thresh_binary)} objects\")\n",
    "print(f\"Otsu: {count_objects(otsu_thresh)} objects\")\n",
    "print(f\"Otsu Inverted: {count_objects(otsu_thresh_inv)} objects\")\n",
    "print(f\"Adaptive: {count_objects(adaptive_mean)} objects\")\n",
    "\n",
    "# Choose the best threshold (usually the one that detects reasonable number of objects)\n",
    "thresholded_img = otsu_thresh_inv  # Often works well for bright objects on dark background\n",
    "print(\"âœ… Selected Otsu inverted thresholding for object detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad114ef",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 4: Morphological Operations - Clean Up Binary Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4.1 Define Kernels\n",
    "# ========================================\n",
    "kernel_small = np.ones((3, 3), np.uint8)\n",
    "kernel_medium = np.ones((5, 5), np.uint8)\n",
    "kernel_large = np.ones((7, 7), np.uint8)\n",
    "kernel_ellipse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "\n",
    "# ========================================\n",
    "# 4.2 Basic Morphological Operations\n",
    "# ========================================\n",
    "# Remove small noise with opening (erosion followed by dilation)\n",
    "opening = cv2.morphologyEx(thresholded_img, cv2.MORPH_OPEN, kernel_small)\n",
    "\n",
    "# Fill small holes with closing (dilation followed by erosion)\n",
    "closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel_medium)\n",
    "\n",
    "# Further noise removal\n",
    "final_opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel_ellipse)\n",
    "\n",
    "# ========================================\n",
    "# 4.3 Advanced Morphological Operations\n",
    "# ========================================\n",
    "# Morphological gradient for edge detection\n",
    "gradient = cv2.morphologyEx(thresholded_img, cv2.MORPH_GRADIENT, kernel_small)\n",
    "\n",
    "# Top hat for small bright details\n",
    "tophat = cv2.morphologyEx(thresholded_img, cv2.MORPH_TOPHAT, kernel_medium)\n",
    "\n",
    "print(\"ðŸ”§ Morphological Cleanup Results:\")\n",
    "print(f\"Original objects: {count_objects(thresholded_img)}\")\n",
    "print(f\"After opening: {count_objects(opening)}\")\n",
    "print(f\"After closing: {count_objects(closing)}\")\n",
    "print(f\"Final cleaned: {count_objects(final_opening)}\")\n",
    "\n",
    "show_results(\n",
    "    [thresholded_img, opening, closing, final_opening, gradient, tophat],\n",
    "    ['Original Binary', 'Opening', 'Closing', 'Final Cleaned', 'Gradient', 'Top Hat'],\n",
    "    rows=2, cols=3\n",
    ")\n",
    "\n",
    "# Use the cleaned binary image for contour detection\n",
    "cleaned_binary = final_opening\n",
    "print(\"âœ… Binary mask cleaned and ready for contour detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a7a3b",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 5: Contour Detection and Object Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379159e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5.1 Find Contours\n",
    "# ========================================\n",
    "contours, hierarchy = cv2.findContours(cleaned_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Filter contours by area to remove noise\n",
    "min_area = 100\n",
    "significant_contours = [c for c in contours if cv2.contourArea(c) > min_area]\n",
    "\n",
    "print(f\"ðŸŽ¯ Contour Detection Results:\")\n",
    "print(f\"Total contours found: {len(contours)}\")\n",
    "print(f\"Significant contours (area > {min_area}): {len(significant_contours)}\")\n",
    "\n",
    "# ========================================\n",
    "# 5.2 Draw Contours and Bounding Boxes\n",
    "# ========================================\n",
    "# Create visualization images\n",
    "contour_img = cv2.cvtColor(cleaned_binary, cv2.COLOR_GRAY2BGR)\n",
    "bbox_img = rgb_img.copy()\n",
    "analysis_img = rgb_img.copy()\n",
    "\n",
    "# Draw contours in different colors\n",
    "cv2.drawContours(contour_img, significant_contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# ========================================\n",
    "# 5.3 Object Analysis\n",
    "# ========================================\n",
    "object_data = []\n",
    "\n",
    "for i, contour in enumerate(significant_contours):\n",
    "    # Calculate properties\n",
    "    area = cv2.contourArea(contour)\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    \n",
    "    # Bounding rectangle\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    cv2.rectangle(bbox_img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    cv2.putText(bbox_img, f'Obj {i+1}', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "    \n",
    "    # Minimum enclosing circle\n",
    "    (cx, cy), radius = cv2.minEnclosingCircle(contour)\n",
    "    center = (int(cx), int(cy))\n",
    "    radius = int(radius)\n",
    "    cv2.circle(analysis_img, center, radius, (0, 255, 255), 2)\n",
    "    cv2.circle(analysis_img, center, 2, (0, 0, 255), -1)\n",
    "    \n",
    "    # Fit ellipse (if contour has enough points)\n",
    "    if len(contour) >= 5:\n",
    "        ellipse = cv2.fitEllipse(contour)\n",
    "        cv2.ellipse(analysis_img, ellipse, (255, 0, 255), 1)\n",
    "    \n",
    "    # Calculate additional properties\n",
    "    aspect_ratio = float(w) / h\n",
    "    extent = float(area) / (w * h)\n",
    "    solidity = float(area) / cv2.contourArea(cv2.convexHull(contour))\n",
    "    \n",
    "    object_data.append({\n",
    "        'id': i + 1,\n",
    "        'area': area,\n",
    "        'perimeter': perimeter,\n",
    "        'aspect_ratio': aspect_ratio,\n",
    "        'extent': extent,\n",
    "        'solidity': solidity,\n",
    "        'center': center\n",
    "    })\n",
    "    \n",
    "    print(f\"Object {i+1}:\")\n",
    "    print(f\"  Area: {area:.0f} pixels\")\n",
    "    print(f\"  Perimeter: {perimeter:.1f} pixels\")\n",
    "    print(f\"  Aspect Ratio: {aspect_ratio:.2f}\")\n",
    "    print(f\"  Extent: {extent:.2f}\")\n",
    "    print(f\"  Solidity: {solidity:.2f}\")\n",
    "    print(f\"  Center: {center}\")\n",
    "    print()\n",
    "\n",
    "show_results(\n",
    "    [rgb_img, cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB), bbox_img, analysis_img],\n",
    "    ['Original Image', 'Detected Contours', 'Bounding Boxes', 'Shape Analysis'],\n",
    "    rows=2, cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85fa3e",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 6: Object Classification and Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c04d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6.1 Simple Object Classification\n",
    "# ========================================\n",
    "def classify_object(obj_data):\n",
    "    \"\"\"Simple classification based on shape properties\"\"\"\n",
    "    aspect_ratio = obj_data['aspect_ratio']\n",
    "    extent = obj_data['extent']\n",
    "    solidity = obj_data['solidity']\n",
    "    \n",
    "    if extent > 0.9 and solidity > 0.9:\n",
    "        if 0.8 < aspect_ratio < 1.2:\n",
    "            return \"Square/Rectangle\"\n",
    "        else:\n",
    "            return \"Rectangle\"\n",
    "    elif solidity > 0.8 and extent < 0.8:\n",
    "        return \"Circle/Ellipse\"\n",
    "    elif solidity < 0.8:\n",
    "        return \"Complex Shape\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Classify all detected objects\n",
    "classified_img = rgb_img.copy()\n",
    "\n",
    "print(\"ðŸ“Š Object Classification Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, obj in enumerate(object_data):\n",
    "    classification = classify_object(obj)\n",
    "    obj['classification'] = classification\n",
    "    \n",
    "    # Draw classification on image\n",
    "    center = obj['center']\n",
    "    cv2.putText(classified_img, f\"{obj['id']}: {classification}\", \n",
    "                (center[0]-50, center[1]+20), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)\n",
    "    \n",
    "    print(f\"Object {obj['id']}: {classification}\")\n",
    "    print(f\"  Properties: AR={obj['aspect_ratio']:.2f}, Ext={obj['extent']:.2f}, Sol={obj['solidity']:.2f}\")\n",
    "\n",
    "# ========================================\n",
    "# 6.2 Create Object Masks\n",
    "# ========================================\n",
    "individual_masks = []\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]\n",
    "\n",
    "colored_objects = np.zeros_like(rgb_img)\n",
    "\n",
    "for i, contour in enumerate(significant_contours):\n",
    "    # Create individual mask for each object\n",
    "    mask = np.zeros(cleaned_binary.shape, dtype=np.uint8)\n",
    "    cv2.drawContours(mask, [contour], -1, 255, -1)\n",
    "    individual_masks.append(mask)\n",
    "    \n",
    "    # Color each object differently\n",
    "    color = colors[i % len(colors)]\n",
    "    colored_objects[mask > 0] = color\n",
    "\n",
    "# ========================================\n",
    "# 6.3 Final Results Summary\n",
    "# ========================================\n",
    "print(f\"\\nðŸŽ¯ Detection Pipeline Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total objects detected: {len(significant_contours)}\")\n",
    "print(f\"Average object area: {np.mean([obj['area'] for obj in object_data]):.0f} pixels\")\n",
    "print(f\"Largest object area: {max([obj['area'] for obj in object_data]):.0f} pixels\")\n",
    "print(f\"Smallest object area: {min([obj['area'] for obj in object_data]):.0f} pixels\")\n",
    "\n",
    "# Count classifications\n",
    "classifications = [obj['classification'] for obj in object_data]\n",
    "unique_classes = set(classifications)\n",
    "for cls in unique_classes:\n",
    "    count = classifications.count(cls)\n",
    "    print(f\"{cls}: {count} objects\")\n",
    "\n",
    "show_results(\n",
    "    [rgb_img, cleaned_binary, classified_img, colored_objects],\n",
    "    ['Original Image', 'Final Binary Mask', 'Classified Objects', 'Colored Objects'],\n",
    "    rows=2, cols=2\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Object Detection and Segmentation Pipeline Completed Successfully!\")\n",
    "print(\"ðŸ“š This pipeline demonstrates:\")\n",
    "print(\"   - Image preprocessing and enhancement\")\n",
    "print(\"   - Thresholding techniques for segmentation\")\n",
    "print(\"   - Morphological operations for cleanup\")\n",
    "print(\"   - Contour detection and analysis\")\n",
    "print(\"   - Object property extraction and classification\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
