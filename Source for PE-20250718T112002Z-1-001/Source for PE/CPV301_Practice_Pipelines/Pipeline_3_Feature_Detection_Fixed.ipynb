{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcac47a2",
   "metadata": {},
   "source": [
    "# üîç Pipeline 3: Feature Detection and Description\n",
    "## Advanced Feature Analysis for Object Recognition\n",
    "\n",
    "### üéØ Problem Statement:\n",
    "**\"Detect, describe, and match features in images for object recognition and tracking\"**\n",
    "\n",
    "### üìã Solution Approach:\n",
    "1. **Corner Detection** ‚Üí Find interest points using Harris and FAST\n",
    "2. **Keypoint Detection** ‚Üí Extract SIFT and SURF features\n",
    "3. **Feature Description** ‚Üí Create robust descriptors\n",
    "4. **Feature Matching** ‚Üí Match features between images\n",
    "5. **Geometric Verification** ‚Üí Filter matches using RANSAC\n",
    "6. **Object Recognition** ‚Üí Identify objects in scene\n",
    "\n",
    "### üîß Techniques Used:\n",
    "- Harris Corner Detection\n",
    "- FAST corner detection\n",
    "- SIFT (Scale-Invariant Feature Transform)\n",
    "- SURF (Speeded-Up Robust Features)\n",
    "- ORB (Oriented FAST and Rotated BRIEF)\n",
    "- Feature matching and homography estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad547f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def show_results(images, titles, rows=2, cols=3, figsize=(15, 10)):\n",
    "    \"\"\"Display multiple images in a grid\"\"\"\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten() if rows * cols > 1 else [axes]\n",
    "    \n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if len(img.shape) == 3:\n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_test_images():\n",
    "    \"\"\"Create test images with various features\"\"\"\n",
    "    # Image 1: Geometric patterns\n",
    "    img1 = np.zeros((400, 400, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Add rectangles\n",
    "    cv2.rectangle(img1, (50, 50), (150, 150), (200, 100, 50), -1)\n",
    "    cv2.rectangle(img1, (200, 80), (350, 180), (100, 200, 150), -1)\n",
    "    \n",
    "    # Add circles\n",
    "    cv2.circle(img1, (100, 250), 50, (150, 50, 200), -1)\n",
    "    cv2.circle(img1, (300, 300), 30, (50, 150, 100), -1)\n",
    "    \n",
    "    # Add triangles\n",
    "    pts = np.array([[250, 200], [200, 280], [300, 280]], np.int32)\n",
    "    cv2.fillPoly(img1, [pts], (200, 200, 50))\n",
    "    \n",
    "    # Add some texture lines\n",
    "    for i in range(0, 400, 20):\n",
    "        cv2.line(img1, (i, 0), (i, 30), (180, 180, 180), 1)\n",
    "        cv2.line(img1, (0, i), (30, i), (180, 180, 180), 1)\n",
    "    \n",
    "    # Image 2: Transformed version\n",
    "    # Apply rotation and scaling\n",
    "    center = (200, 200)\n",
    "    angle = 15\n",
    "    scale = 0.8\n",
    "    M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    img2 = cv2.warpAffine(img1, M, (400, 400))\n",
    "    \n",
    "    # Add some noise\n",
    "    noise = np.random.normal(0, 10, img2.shape).astype(np.int16)\n",
    "    img2 = np.clip(img2.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return img1, img2\n",
    "\n",
    "def draw_keypoints(img, keypoints, color=(0, 255, 0)):\n",
    "    \"\"\"Draw keypoints on image\"\"\"\n",
    "    result = img.copy()\n",
    "    for kp in keypoints:\n",
    "        x, y = int(kp.pt[0]), int(kp.pt[1])\n",
    "        cv2.circle(result, (x, y), 3, color, -1)\n",
    "        # Draw orientation if available\n",
    "        if hasattr(kp, 'angle') and kp.angle != -1:\n",
    "            angle = np.radians(kp.angle)\n",
    "            x2 = int(x + 10 * np.cos(angle))\n",
    "            y2 = int(y + 10 * np.sin(angle))\n",
    "            cv2.line(result, (x, y), (x2, y2), color, 1)\n",
    "    return result\n",
    "\n",
    "print(\"üîç Feature Detection and Description Pipeline Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457bb08",
   "metadata": {},
   "source": [
    "## üìñ Step 1: Load Images and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load real images first\n",
    "try:\n",
    "    img1 = cv2.imread('D:/FPT_Material/Sem 4/CPV301/Source for PE/Feature Detection and Description/Image/chessboard.jpg')\n",
    "    img2 = cv2.imread('D:/FPT_Material/Sem 4/CPV301/Source for PE/Feature Detection and Description/Image/home.jpg')\n",
    "    \n",
    "    if img1 is None or img2 is None:\n",
    "        raise FileNotFoundError(\"Images not found\")\n",
    "    \n",
    "    # Resize images if they're too large\n",
    "    max_size = 500\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    if max(h1, w1) > max_size:\n",
    "        scale = max_size / max(h1, w1)\n",
    "        new_w1, new_h1 = int(w1 * scale), int(h1 * scale)\n",
    "        img1 = cv2.resize(img1, (new_w1, new_h1))\n",
    "    \n",
    "    h2, w2 = img2.shape[:2]\n",
    "    if max(h2, w2) > max_size:\n",
    "        scale = max_size / max(h2, w2)\n",
    "        new_w2, new_h2 = int(w2 * scale), int(h2 * scale)\n",
    "        img2 = cv2.resize(img2, (new_w2, new_h2))\n",
    "    \n",
    "    print(\"‚úÖ Loaded images from files\")\n",
    "    \n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Creating synthetic test images...\")\n",
    "    img1, img2 = create_test_images()\n",
    "\n",
    "# Convert to grayscale for feature detection\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "print(f\"üìä Image Information:\")\n",
    "print(f\"Image 1 shape: {img1.shape}\")\n",
    "print(f\"Image 2 shape: {img2.shape}\")\n",
    "\n",
    "# Show original images\n",
    "show_results(\n",
    "    [cv2.cvtColor(img1, cv2.COLOR_BGR2RGB), cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)],\n",
    "    ['Image 1', 'Image 2'],\n",
    "    rows=1, cols=2, figsize=(12, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6379b70",
   "metadata": {},
   "source": [
    "## üéØ Step 2: Corner Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2.1 Harris Corner Detection\n",
    "# ========================================\n",
    "\n",
    "def harris_corner_detection(img, k=0.04, threshold=0.01):\n",
    "    \"\"\"Harris corner detection with detailed analysis\"\"\"\n",
    "    # Convert to float\n",
    "    img_float = np.float32(img)\n",
    "    \n",
    "    # Calculate Harris response\n",
    "    harris_response = cv2.cornerHarris(img_float, 2, 3, k)\n",
    "    \n",
    "    # Threshold for corner detection\n",
    "    corners = harris_response > threshold * harris_response.max()\n",
    "    \n",
    "    # Find corner coordinates\n",
    "    corner_coords = np.where(corners)\n",
    "    corner_points = list(zip(corner_coords[1], corner_coords[0]))  # (x, y) format\n",
    "    \n",
    "    return harris_response, corners, corner_points\n",
    "\n",
    "# Apply Harris corner detection to both images\n",
    "harris1, corners1, points1 = harris_corner_detection(gray1)\n",
    "harris2, corners2, points2 = harris_corner_detection(gray2)\n",
    "\n",
    "print(f\"üéØ Harris Corner Detection Results:\")\n",
    "print(f\"Image 1: Found {len(points1)} corners\")\n",
    "print(f\"Image 2: Found {len(points2)} corners\")\n",
    "\n",
    "# Visualize Harris corners\n",
    "img1_harris = img1.copy()\n",
    "img2_harris = img2.copy()\n",
    "\n",
    "for x, y in points1:\n",
    "    cv2.circle(img1_harris, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "for x, y in points2:\n",
    "    cv2.circle(img2_harris, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "# ========================================\n",
    "# 2.2 FAST Corner Detection\n",
    "# ========================================\n",
    "\n",
    "# Create FAST detector\n",
    "fast = cv2.FastFeatureDetector_create(threshold=50, nonmaxSuppression=True)\n",
    "\n",
    "# Detect FAST corners\n",
    "fast_kp1 = fast.detect(gray1, None)\n",
    "fast_kp2 = fast.detect(gray2, None)\n",
    "\n",
    "print(f\"\\n‚ö° FAST Corner Detection Results:\")\n",
    "print(f\"Image 1: Found {len(fast_kp1)} FAST corners\")\n",
    "print(f\"Image 2: Found {len(fast_kp2)} FAST corners\")\n",
    "\n",
    "# Visualize FAST corners\n",
    "img1_fast = draw_keypoints(img1, fast_kp1, (255, 0, 0))\n",
    "img2_fast = draw_keypoints(img2, fast_kp2, (255, 0, 0))\n",
    "\n",
    "# ========================================\n",
    "# 2.3 goodFeaturesToTrack (Shi-Tomasi)\n",
    "# ========================================\n",
    "\n",
    "# Shi-Tomasi corner detection\n",
    "shi_corners1 = cv2.goodFeaturesToTrack(gray1, maxCorners=100, qualityLevel=0.01, minDistance=10)\n",
    "shi_corners2 = cv2.goodFeaturesToTrack(gray2, maxCorners=100, qualityLevel=0.01, minDistance=10)\n",
    "\n",
    "print(f\"\\nüìê Shi-Tomasi Corner Detection Results:\")\n",
    "if shi_corners1 is not None:\n",
    "    print(f\"Image 1: Found {len(shi_corners1)} Shi-Tomasi corners\")\n",
    "else:\n",
    "    print(f\"Image 1: No Shi-Tomasi corners found\")\n",
    "\n",
    "if shi_corners2 is not None:\n",
    "    print(f\"Image 2: Found {len(shi_corners2)} Shi-Tomasi corners\")\n",
    "else:\n",
    "    print(f\"Image 2: No Shi-Tomasi corners found\")\n",
    "\n",
    "# Visualize Shi-Tomasi corners\n",
    "img1_shi = img1.copy()\n",
    "img2_shi = img2.copy()\n",
    "\n",
    "if shi_corners1 is not None:\n",
    "    for corner in shi_corners1:\n",
    "        x, y = corner.ravel().astype(int)\n",
    "        cv2.circle(img1_shi, (x, y), 3, (0, 0, 255), -1)\n",
    "\n",
    "if shi_corners2 is not None:\n",
    "    for corner in shi_corners2:\n",
    "        x, y = corner.ravel().astype(int)\n",
    "        cv2.circle(img2_shi, (x, y), 3, (0, 0, 255), -1)\n",
    "\n",
    "# Show corner detection results\n",
    "show_results(\n",
    "    [cv2.cvtColor(img1_harris, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(img1_fast, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(img1_shi, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(img2_harris, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(img2_fast, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(img2_shi, cv2.COLOR_BGR2RGB)],\n",
    "    ['Harris Corners', 'FAST Corners', 'Shi-Tomasi', \n",
    "     'Harris Corners', 'FAST Corners', 'Shi-Tomasi'],\n",
    "    rows=2, cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c282734",
   "metadata": {},
   "source": [
    "## üîç Step 3: Keypoint Detection and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2332098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3.1 SIFT Feature Detection\n",
    "# ========================================\n",
    "\n",
    "try:\n",
    "    # Create SIFT detector\n",
    "    sift = cv2.SIFT_create(nfeatures=500)\n",
    "    \n",
    "    # Detect and compute SIFT features\n",
    "    sift_kp1, sift_desc1 = sift.detectAndCompute(gray1, None)\n",
    "    sift_kp2, sift_desc2 = sift.detectAndCompute(gray2, None)\n",
    "    \n",
    "    print(f\"üîç SIFT Feature Detection Results:\")\n",
    "    print(f\"Image 1: {len(sift_kp1)} keypoints, descriptor shape: {sift_desc1.shape if sift_desc1 is not None else 'None'}\")\n",
    "    print(f\"Image 2: {len(sift_kp2)} keypoints, descriptor shape: {sift_desc2.shape if sift_desc2 is not None else 'None'}\")\n",
    "    \n",
    "    sift_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è SIFT not available: {e}\")\n",
    "    sift_available = False\n",
    "    sift_kp1, sift_desc1 = [], None\n",
    "    sift_kp2, sift_desc2 = [], None\n",
    "\n",
    "# ========================================\n",
    "# 3.2 ORB Feature Detection\n",
    "# ========================================\n",
    "\n",
    "# Create ORB detector\n",
    "orb = cv2.ORB_create(nfeatures=500)\n",
    "\n",
    "# Detect and compute ORB features\n",
    "orb_kp1, orb_desc1 = orb.detectAndCompute(gray1, None)\n",
    "orb_kp2, orb_desc2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "print(f\"\\nüéØ ORB Feature Detection Results:\")\n",
    "print(f\"Image 1: {len(orb_kp1)} keypoints, descriptor shape: {orb_desc1.shape if orb_desc1 is not None else 'None'}\")\n",
    "print(f\"Image 2: {len(orb_kp2)} keypoints, descriptor shape: {orb_desc2.shape if orb_desc2 is not None else 'None'}\")\n",
    "\n",
    "# ========================================\n",
    "# 3.3 AKAZE Feature Detection\n",
    "# ========================================\n",
    "\n",
    "try:\n",
    "    # Create AKAZE detector\n",
    "    akaze = cv2.AKAZE_create()\n",
    "    \n",
    "    # Detect and compute AKAZE features\n",
    "    akaze_kp1, akaze_desc1 = akaze.detectAndCompute(gray1, None)\n",
    "    akaze_kp2, akaze_desc2 = akaze.detectAndCompute(gray2, None)\n",
    "    \n",
    "    print(f\"\\nüåü AKAZE Feature Detection Results:\")\n",
    "    print(f\"Image 1: {len(akaze_kp1)} keypoints, descriptor shape: {akaze_desc1.shape if akaze_desc1 is not None else 'None'}\")\n",
    "    print(f\"Image 2: {len(akaze_kp2)} keypoints, descriptor shape: {akaze_desc2.shape if akaze_desc2 is not None else 'None'}\")\n",
    "    \n",
    "    akaze_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è AKAZE error: {e}\")\n",
    "    akaze_available = False\n",
    "    akaze_kp1, akaze_desc1 = [], None\n",
    "    akaze_kp2, akaze_desc2 = [], None\n",
    "\n",
    "# ========================================\n",
    "# 3.4 Visualize Keypoints\n",
    "# ========================================\n",
    "\n",
    "# Draw keypoints with orientations and scales\n",
    "img1_features = []\n",
    "img2_features = []\n",
    "feature_names = []\n",
    "\n",
    "if sift_available and len(sift_kp1) > 0:\n",
    "    img1_sift = cv2.drawKeypoints(img1, sift_kp1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    img2_sift = cv2.drawKeypoints(img2, sift_kp2, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    img1_features.extend([img1_sift])\n",
    "    img2_features.extend([img2_sift])\n",
    "    feature_names.extend(['SIFT'])\n",
    "\n",
    "if len(orb_kp1) > 0:\n",
    "    img1_orb = cv2.drawKeypoints(img1, orb_kp1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    img2_orb = cv2.drawKeypoints(img2, orb_kp2, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    img1_features.extend([img1_orb])\n",
    "    img2_features.extend([img2_orb])\n",
    "    feature_names.extend(['ORB'])\n",
    "\n",
    "if akaze_available and len(akaze_kp1) > 0:\n",
    "    img1_akaze = cv2.drawKeypoints(img1, akaze_kp1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    img2_akaze = cv2.drawKeypoints(img2, akaze_kp2, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    img1_features.extend([img1_akaze])\n",
    "    img2_features.extend([img2_akaze])\n",
    "    feature_names.extend(['AKAZE'])\n",
    "\n",
    "# Show feature detection results\n",
    "if img1_features:\n",
    "    all_images = []\n",
    "    all_titles = []\n",
    "    \n",
    "    for i, name in enumerate(feature_names):\n",
    "        all_images.extend([cv2.cvtColor(img1_features[i], cv2.COLOR_BGR2RGB),\n",
    "                          cv2.cvtColor(img2_features[i], cv2.COLOR_BGR2RGB)])\n",
    "        all_titles.extend([f'{name} - Image 1', f'{name} - Image 2'])\n",
    "    \n",
    "    show_results(all_images, all_titles, rows=len(feature_names), cols=2, figsize=(12, 4*len(feature_names)))\n",
    "\n",
    "# ========================================\n",
    "# 3.5 Feature Analysis\n",
    "# ========================================\n",
    "\n",
    "def analyze_keypoints(keypoints, name):\n",
    "    \"\"\"Analyze keypoint properties\"\"\"\n",
    "    if len(keypoints) == 0:\n",
    "        return\n",
    "    \n",
    "    # Extract properties\n",
    "    scales = [kp.size for kp in keypoints]\n",
    "    responses = [kp.response for kp in keypoints]\n",
    "    \n",
    "    print(f\"\\nüìä {name} Analysis:\")\n",
    "    print(f\"  Count: {len(keypoints)}\")\n",
    "    print(f\"  Scale range: {min(scales):.1f} - {max(scales):.1f}\")\n",
    "    print(f\"  Response range: {min(responses):.3f} - {max(responses):.3f}\")\n",
    "    print(f\"  Average scale: {np.mean(scales):.1f}\")\n",
    "    print(f\"  Average response: {np.mean(responses):.3f}\")\n",
    "\n",
    "if sift_available:\n",
    "    analyze_keypoints(sift_kp1, \"SIFT Image 1\")\n",
    "    analyze_keypoints(sift_kp2, \"SIFT Image 2\")\n",
    "\n",
    "analyze_keypoints(orb_kp1, \"ORB Image 1\")\n",
    "analyze_keypoints(orb_kp2, \"ORB Image 2\")\n",
    "\n",
    "if akaze_available:\n",
    "    analyze_keypoints(akaze_kp1, \"AKAZE Image 1\")\n",
    "    analyze_keypoints(akaze_kp2, \"AKAZE Image 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce8a68",
   "metadata": {},
   "source": [
    "## üîó Step 4: Feature Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4.1 Brute Force Matcher\n",
    "# ========================================\n",
    "\n",
    "def match_features_bf(desc1, desc2, descriptor_type='ORB', ratio_test=True):\n",
    "    \"\"\"Match features using Brute Force matcher\"\"\"\n",
    "    if desc1 is None or desc2 is None or len(desc1) == 0 or len(desc2) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Choose appropriate distance metric\n",
    "    if descriptor_type in ['SIFT', 'SURF', 'AKAZE']:\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "    else:  # ORB, BRIEF, etc.\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "    \n",
    "    if ratio_test:\n",
    "        # Lowe's ratio test\n",
    "        matches = bf.knnMatch(desc1, desc2, k=2)\n",
    "        good_matches = []\n",
    "        for match_pair in matches:\n",
    "            if len(match_pair) == 2:\n",
    "                m, n = match_pair\n",
    "                if m.distance < 0.75 * n.distance:\n",
    "                    good_matches.append(m)\n",
    "        return good_matches\n",
    "    else:\n",
    "        matches = bf.match(desc1, desc2)\n",
    "        return sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# ========================================\n",
    "# 4.2 FLANN Matcher\n",
    "# ========================================\n",
    "\n",
    "def match_features_flann(desc1, desc2, descriptor_type='SIFT'):\n",
    "    \"\"\"Match features using FLANN matcher\"\"\"\n",
    "    if desc1 is None or desc2 is None or len(desc1) == 0 or len(desc2) == 0:\n",
    "        return []\n",
    "    \n",
    "    if descriptor_type in ['SIFT', 'SURF']:\n",
    "        # FLANN parameters for SIFT/SURF\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "    else:\n",
    "        # FLANN parameters for ORB\n",
    "        FLANN_INDEX_LSH = 6\n",
    "        index_params = dict(algorithm=FLANN_INDEX_LSH,\n",
    "                           table_number=6,\n",
    "                           key_size=12,\n",
    "                           multi_probe_level=1)\n",
    "        search_params = dict(checks=50)\n",
    "    \n",
    "    try:\n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = flann.knnMatch(desc1, desc2, k=2)\n",
    "        \n",
    "        # Lowe's ratio test\n",
    "        good_matches = []\n",
    "        for match_pair in matches:\n",
    "            if len(match_pair) == 2:\n",
    "                m, n = match_pair\n",
    "                if m.distance < 0.7 * n.distance:\n",
    "                    good_matches.append(m)\n",
    "        \n",
    "        return good_matches\n",
    "    except Exception as e:\n",
    "        print(f\"FLANN matching failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# ========================================\n",
    "# 4.3 Perform Matching for Each Feature Type\n",
    "# ========================================\n",
    "\n",
    "matching_results = {}\n",
    "\n",
    "# SIFT matching\n",
    "if sift_available and sift_desc1 is not None and sift_desc2 is not None:\n",
    "    sift_matches_bf = match_features_bf(sift_desc1, sift_desc2, 'SIFT')\n",
    "    sift_matches_flann = match_features_flann(sift_desc1, sift_desc2, 'SIFT')\n",
    "    matching_results['SIFT'] = {\n",
    "        'bf': sift_matches_bf,\n",
    "        'flann': sift_matches_flann,\n",
    "        'kp1': sift_kp1,\n",
    "        'kp2': sift_kp2\n",
    "    }\n",
    "    print(f\"üîç SIFT Matching:\")\n",
    "    print(f\"  Brute Force: {len(sift_matches_bf)} matches\")\n",
    "    print(f\"  FLANN: {len(sift_matches_flann)} matches\")\n",
    "\n",
    "# ORB matching\n",
    "if orb_desc1 is not None and orb_desc2 is not None:\n",
    "    orb_matches_bf = match_features_bf(orb_desc1, orb_desc2, 'ORB')\n",
    "    orb_matches_flann = match_features_flann(orb_desc1, orb_desc2, 'ORB')\n",
    "    matching_results['ORB'] = {\n",
    "        'bf': orb_matches_bf,\n",
    "        'flann': orb_matches_flann,\n",
    "        'kp1': orb_kp1,\n",
    "        'kp2': orb_kp2\n",
    "    }\n",
    "    print(f\"\\nüéØ ORB Matching:\")\n",
    "    print(f\"  Brute Force: {len(orb_matches_bf)} matches\")\n",
    "    print(f\"  FLANN: {len(orb_matches_flann)} matches\")\n",
    "\n",
    "# AKAZE matching\n",
    "if akaze_available and akaze_desc1 is not None and akaze_desc2 is not None:\n",
    "    akaze_matches_bf = match_features_bf(akaze_desc1, akaze_desc2, 'AKAZE')\n",
    "    matching_results['AKAZE'] = {\n",
    "        'bf': akaze_matches_bf,\n",
    "        'flann': [],  # FLANN might not work well with AKAZE\n",
    "        'kp1': akaze_kp1,\n",
    "        'kp2': akaze_kp2\n",
    "    }\n",
    "    print(f\"\\nüåü AKAZE Matching:\")\n",
    "    print(f\"  Brute Force: {len(akaze_matches_bf)} matches\")\n",
    "\n",
    "# ========================================\n",
    "# 4.4 Visualize Matches\n",
    "# ========================================\n",
    "\n",
    "def draw_matches_custom(img1, kp1, img2, kp2, matches, max_matches=50):\n",
    "    \"\"\"Draw matches between two images\"\"\"\n",
    "    # Limit number of matches to display\n",
    "    matches_to_draw = matches[:max_matches]\n",
    "    \n",
    "    # Draw matches\n",
    "    img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches_to_draw, None,\n",
    "                                  flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    return img_matches\n",
    "\n",
    "match_images = []\n",
    "match_titles = []\n",
    "\n",
    "for feature_type, results in matching_results.items():\n",
    "    if len(results['bf']) > 0:\n",
    "        img_matches_bf = draw_matches_custom(img1, results['kp1'], img2, results['kp2'], results['bf'])\n",
    "        match_images.append(cv2.cvtColor(img_matches_bf, cv2.COLOR_BGR2RGB))\n",
    "        match_titles.append(f'{feature_type} - Brute Force')\n",
    "    \n",
    "    if len(results['flann']) > 0:\n",
    "        img_matches_flann = draw_matches_custom(img1, results['kp1'], img2, results['kp2'], results['flann'])\n",
    "        match_images.append(cv2.cvtColor(img_matches_flann, cv2.COLOR_BGR2RGB))\n",
    "        match_titles.append(f'{feature_type} - FLANN')\n",
    "\n",
    "if match_images:\n",
    "    # Calculate appropriate rows and cols\n",
    "    n_images = len(match_images)\n",
    "    cols = min(2, n_images)\n",
    "    rows = (n_images + cols - 1) // cols\n",
    "    \n",
    "    show_results(match_images, match_titles, rows=rows, cols=cols, figsize=(15, 6*rows))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No matches found to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4517bf7",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Geometric Verification and Homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5.1 RANSAC Homography Estimation\n",
    "# ========================================\n",
    "\n",
    "def estimate_homography_ransac(kp1, kp2, matches, threshold=5.0):\n",
    "    \"\"\"Estimate homography using RANSAC\"\"\"\n",
    "    if len(matches) < 4:\n",
    "        return None, None, []\n",
    "    \n",
    "    # Extract matching points\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    \n",
    "    # Estimate homography with RANSAC\n",
    "    homography, mask = cv2.findHomography(src_pts, dst_pts, \n",
    "                                         cv2.RANSAC, threshold)\n",
    "    \n",
    "    # Get inlier matches\n",
    "    inlier_matches = [matches[i] for i in range(len(matches)) if mask[i]]\n",
    "    \n",
    "    return homography, mask, inlier_matches\n",
    "\n",
    "# ========================================\n",
    "# 5.2 Apply RANSAC to Best Matching Results\n",
    "# ========================================\n",
    "\n",
    "homography_results = {}\n",
    "\n",
    "for feature_type, results in matching_results.items():\n",
    "    matches = results['bf'] if len(results['bf']) > len(results['flann']) else results['flann']\n",
    "    \n",
    "    if len(matches) >= 4:\n",
    "        H, mask, inliers = estimate_homography_ransac(results['kp1'], results['kp2'], matches)\n",
    "        \n",
    "        homography_results[feature_type] = {\n",
    "            'homography': H,\n",
    "            'inliers': inliers,\n",
    "            'inlier_ratio': len(inliers) / len(matches) if len(matches) > 0 else 0,\n",
    "            'kp1': results['kp1'],\n",
    "            'kp2': results['kp2']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüéØ {feature_type} Homography Results:\")\n",
    "        print(f\"  Total matches: {len(matches)}\")\n",
    "        print(f\"  Inlier matches: {len(inliers)}\")\n",
    "        print(f\"  Inlier ratio: {len(inliers) / len(matches):.2%}\")\n",
    "        \n",
    "        if H is not None:\n",
    "            print(f\"  Homography matrix:\")\n",
    "            for row in H:\n",
    "                print(f\"    [{row[0]:8.4f} {row[1]:8.4f} {row[2]:8.4f}]\")\n",
    "\n",
    "# ========================================\n",
    "# 5.3 Visualize Inlier Matches\n",
    "# ========================================\n",
    "\n",
    "inlier_images = []\n",
    "inlier_titles = []\n",
    "\n",
    "for feature_type, results in homography_results.items():\n",
    "    if len(results['inliers']) > 0:\n",
    "        img_inliers = draw_matches_custom(img1, results['kp1'], img2, results['kp2'], \n",
    "                                        results['inliers'], max_matches=50)\n",
    "        inlier_images.append(cv2.cvtColor(img_inliers, cv2.COLOR_BGR2RGB))\n",
    "        inlier_titles.append(f'{feature_type} - Inlier Matches ({len(results[\"inliers\"])})')\n",
    "\n",
    "if inlier_images:\n",
    "    show_results(inlier_images, inlier_titles, \n",
    "                rows=len(inlier_images), cols=1, figsize=(15, 6*len(inlier_images)))\n",
    "\n",
    "# ========================================\n",
    "# 5.4 Object Detection Using Homography\n",
    "# ========================================\n",
    "\n",
    "def detect_object_with_homography(img1, img2, homography):\n",
    "    \"\"\"Detect object in img2 using homography from img1\"\"\"\n",
    "    if homography is None:\n",
    "        return None\n",
    "    \n",
    "    # Get corners of img1\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    corners1 = np.float32([[0, 0], [w1, 0], [w1, h1], [0, h1]]).reshape(-1, 1, 2)\n",
    "    \n",
    "    # Transform corners to img2 coordinates\n",
    "    corners2 = cv2.perspectiveTransform(corners1, homography)\n",
    "    \n",
    "    # Draw the detected object boundary\n",
    "    img2_with_detection = img2.copy()\n",
    "    cv2.polylines(img2_with_detection, [np.int32(corners2)], True, (0, 255, 0), 3)\n",
    "    \n",
    "    return img2_with_detection, corners2\n",
    "\n",
    "# Apply object detection for best homography\n",
    "best_feature_type = None\n",
    "best_inlier_ratio = 0\n",
    "\n",
    "for feature_type, results in homography_results.items():\n",
    "    if results['inlier_ratio'] > best_inlier_ratio and len(results['inliers']) >= 10:\n",
    "        best_inlier_ratio = results['inlier_ratio']\n",
    "        best_feature_type = feature_type\n",
    "\n",
    "if best_feature_type:\n",
    "    H_best = homography_results[best_feature_type]['homography']\n",
    "    detection_result = detect_object_with_homography(img1, img2, H_best)\n",
    "    \n",
    "    if detection_result:\n",
    "        img2_detected, corners = detection_result\n",
    "        \n",
    "        print(f\"\\nüéØ Object Detection Results (using {best_feature_type}):\")\n",
    "        print(f\"  Best feature type: {best_feature_type}\")\n",
    "        print(f\"  Inlier ratio: {best_inlier_ratio:.2%}\")\n",
    "        print(f\"  Detected corners in image 2:\")\n",
    "        for i, corner in enumerate(corners):\n",
    "            print(f\"    Corner {i+1}: ({corner[0][0]:.1f}, {corner[0][1]:.1f})\")\n",
    "        \n",
    "        show_results(\n",
    "            [cv2.cvtColor(img1, cv2.COLOR_BGR2RGB), cv2.cvtColor(img2_detected, cv2.COLOR_BGR2RGB)],\n",
    "            ['Template Image', f'Detected Object ({best_feature_type})'],\n",
    "            rows=1, cols=2, figsize=(15, 6)\n",
    "        )\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No reliable homography found for object detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed429f7",
   "metadata": {},
   "source": [
    "## üìä Step 6: Performance Analysis and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6.1 Feature Detection Performance Summary\n",
    "# ========================================\n",
    "\n",
    "print(\"üìä Feature Detection and Matching Performance Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for feature_type, results in matching_results.items():\n",
    "    total_kp1 = len(results['kp1'])\n",
    "    total_kp2 = len(results['kp2'])\n",
    "    total_matches = len(results['bf'])\n",
    "    \n",
    "    if feature_type in homography_results:\n",
    "        inliers = len(homography_results[feature_type]['inliers'])\n",
    "        inlier_ratio = homography_results[feature_type]['inlier_ratio']\n",
    "        homography_found = homography_results[feature_type]['homography'] is not None\n",
    "    else:\n",
    "        inliers = 0\n",
    "        inlier_ratio = 0\n",
    "        homography_found = False\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Feature': feature_type,\n",
    "        'Keypoints 1': total_kp1,\n",
    "        'Keypoints 2': total_kp2,\n",
    "        'Total Matches': total_matches,\n",
    "        'Inliers': inliers,\n",
    "        'Inlier Ratio': f\"{inlier_ratio:.1%}\",\n",
    "        'Homography': '‚úì' if homography_found else '‚úó'\n",
    "    })\n",
    "\n",
    "# Print summary table\n",
    "if summary_data:\n",
    "    # Print header\n",
    "    headers = ['Feature', 'KP1', 'KP2', 'Matches', 'Inliers', 'Ratio', 'Homography']\n",
    "    print(f\"{'Feature':<8} {'KP1':<5} {'KP2':<5} {'Matches':<8} {'Inliers':<8} {'Ratio':<8} {'Homography':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for data in summary_data:\n",
    "        print(f\"{data['Feature']:<8} {data['Keypoints 1']:<5} {data['Keypoints 2']:<5} \"\n",
    "              f\"{data['Total Matches']:<8} {data['Inliers']:<8} {data['Inlier Ratio']:<8} {data['Homography']:<10}\")\n",
    "\n",
    "# ========================================\n",
    "# 6.2 Feature Quality Analysis\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüîç Feature Quality Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Analyze repeatability and matching performance\n",
    "for feature_type, results in matching_results.items():\n",
    "    kp1_count = len(results['kp1'])\n",
    "    kp2_count = len(results['kp2'])\n",
    "    matches_count = len(results['bf'])\n",
    "    \n",
    "    if kp1_count > 0 and kp2_count > 0:\n",
    "        matching_rate = matches_count / min(kp1_count, kp2_count)\n",
    "        print(f\"\\n{feature_type}:\")\n",
    "        print(f\"  Keypoint density: {(kp1_count + kp2_count) / 2:.1f} per image\")\n",
    "        print(f\"  Matching rate: {matching_rate:.1%}\")\n",
    "        \n",
    "        if feature_type in homography_results:\n",
    "            inlier_ratio = homography_results[feature_type]['inlier_ratio']\n",
    "            print(f\"  Geometric consistency: {inlier_ratio:.1%}\")\n",
    "            \n",
    "            # Overall score (combination of matching rate and geometric consistency)\n",
    "            overall_score = (matching_rate * 0.3 + inlier_ratio * 0.7) * 100\n",
    "            print(f\"  Overall quality score: {overall_score:.1f}/100\")\n",
    "\n",
    "# ========================================\n",
    "# 6.3 Recommendations\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüí° Recommendations for Feature Detection:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if best_feature_type:\n",
    "    print(f\"‚úÖ Best performing method: {best_feature_type}\")\n",
    "    print(f\"   Reason: Highest inlier ratio ({best_inlier_ratio:.1%}) with sufficient matches\")\n",
    "\n",
    "print(f\"\\nüìö Method Characteristics:\")\n",
    "if sift_available:\n",
    "    print(f\"‚Ä¢ SIFT: Scale and rotation invariant, best for textured images\")\n",
    "print(f\"‚Ä¢ ORB: Fast, good for real-time applications, binary descriptors\")\n",
    "if akaze_available:\n",
    "    print(f\"‚Ä¢ AKAZE: Good balance of speed and accuracy, nonlinear scale space\")\n",
    "\n",
    "print(f\"\\nüéØ Use Case Guidelines:\")\n",
    "print(f\"‚Ä¢ Real-time applications: Use ORB\")\n",
    "print(f\"‚Ä¢ High accuracy needed: Use SIFT (if available)\")\n",
    "print(f\"‚Ä¢ Planar objects: Any method works well\")\n",
    "print(f\"‚Ä¢ Textured scenes: SIFT or AKAZE preferred\")\n",
    "print(f\"‚Ä¢ Low-texture scenes: Increase detector thresholds\")\n",
    "\n",
    "print(f\"\\nüîß Parameter Tuning Tips:\")\n",
    "print(f\"‚Ä¢ Increase nfeatures for more keypoints\")\n",
    "print(f\"‚Ä¢ Lower thresholds for more features (but more noise)\")\n",
    "print(f\"‚Ä¢ Use ratio test (0.7-0.8) for better matches\")\n",
    "print(f\"‚Ä¢ RANSAC threshold: 1-5 pixels typical\")\n",
    "\n",
    "print(f\"\\nüéâ Feature Detection Pipeline Completed Successfully!\")\n",
    "print(f\"üìà This pipeline demonstrated:\")\n",
    "print(f\"   - Multiple feature detection algorithms\")\n",
    "print(f\"   - Different matching strategies\")\n",
    "print(f\"   - Geometric verification with RANSAC\")\n",
    "print(f\"   - Object detection using homography\")\n",
    "print(f\"   - Performance analysis and comparison\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
