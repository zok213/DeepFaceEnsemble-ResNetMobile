{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec0bd4a",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Pipeline 2: Image Enhancement and Restoration\n",
    "## Advanced Image Processing for Quality Improvement\n",
    "\n",
    "### ðŸŽ¯ Problem Statement:\n",
    "**\"Given a low-quality image, enhance it using various image processing techniques\"**\n",
    "\n",
    "### ðŸ“‹ Solution Approach:\n",
    "1. **Image Analysis** â†’ Identify quality issues\n",
    "2. **Noise Reduction** â†’ Remove different types of noise\n",
    "3. **Contrast Enhancement** â†’ Improve visibility\n",
    "4. **Sharpening** â†’ Enhance details\n",
    "5. **Color Correction** â†’ Fix color issues\n",
    "6. **Final Optimization** â†’ Combine best techniques\n",
    "\n",
    "### ðŸ”§ Techniques Used:\n",
    "- Histogram analysis and equalization\n",
    "- Various denoising filters\n",
    "- Contrast stretching and CLAHE\n",
    "- Unsharp masking and sharpening\n",
    "- White balancing algorithms\n",
    "- Frequency domain filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b3b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "def show_results(images, titles, rows=2, cols=3, figsize=(15, 10)):\n",
    "    \"\"\"Display multiple images in a grid\"\"\"\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten() if rows * cols > 1 else [axes]\n",
    "    \n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if len(img.shape) == 3:\n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_degraded_image():\n",
    "    \"\"\"Create a test image with various quality issues\"\"\"\n",
    "    # Start with a simple test pattern\n",
    "    img = np.zeros((300, 300, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Add geometric patterns\n",
    "    cv2.rectangle(img, (50, 50), (150, 150), (180, 180, 180), -1)\n",
    "    cv2.circle(img, (200, 80), 40, (120, 120, 120), -1)\n",
    "    cv2.rectangle(img, (80, 180), (180, 250), (160, 160, 160), -1)\n",
    "    \n",
    "    # Add some text-like features\n",
    "    for i in range(5):\n",
    "        cv2.line(img, (200, 150 + i*10), (270, 150 + i*10), (100, 100, 100), 2)\n",
    "    \n",
    "    # Simulate various degradations\n",
    "    \n",
    "    # 1. Add Gaussian noise\n",
    "    noise = np.random.normal(0, 25, img.shape).astype(np.int16)\n",
    "    img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # 2. Add salt and pepper noise\n",
    "    mask = np.random.random(img.shape[:2])\n",
    "    img[mask < 0.01] = 0     # Salt noise\n",
    "    img[mask > 0.99] = 255   # Pepper noise\n",
    "    \n",
    "    # 3. Reduce contrast\n",
    "    img = np.clip(img * 0.6 + 50, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # 4. Add blur (motion blur simulation)\n",
    "    kernel = np.zeros((9, 9))\n",
    "    kernel[4, :] = 1/9\n",
    "    img = cv2.filter2D(img, -1, kernel)\n",
    "    \n",
    "    # 5. Color cast (simulating poor white balance)\n",
    "    img[:, :, 0] = np.clip(img[:, :, 0] * 1.2, 0, 255)  # Blue cast\n",
    "    \n",
    "    return img\n",
    "\n",
    "def analyze_image_quality(img):\n",
    "    \"\"\"Analyze image quality metrics\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img\n",
    "    \n",
    "    # Calculate various quality metrics\n",
    "    mean_brightness = np.mean(gray)\n",
    "    contrast = np.std(gray)\n",
    "    dynamic_range = gray.max() - gray.min()\n",
    "    \n",
    "    # Estimate noise using Laplacian variance\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    \n",
    "    # Edge density\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    edge_density = np.sum(edges > 0) / edges.size\n",
    "    \n",
    "    return {\n",
    "        'brightness': mean_brightness,\n",
    "        'contrast': contrast,\n",
    "        'dynamic_range': dynamic_range,\n",
    "        'noise_estimate': laplacian_var,\n",
    "        'edge_density': edge_density\n",
    "    }\n",
    "\n",
    "print(\"ðŸŒŸ Image Enhancement and Restoration Pipeline Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75d1f9",
   "metadata": {},
   "source": [
    "## ðŸ“– Step 1: Load and Analyze Image Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create test image\n",
    "try:\n",
    "    img = cv2.imread('D:/FPT_Material/Sem 4/CPV301/Source for PE/Image/low_contrast_image2.png')\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(\"Image not found\")\n",
    "    print(\"âœ… Loaded image from file\")\n",
    "except:\n",
    "    print(\"âš ï¸ Creating degraded test image...\")\n",
    "    img = create_degraded_image()\n",
    "\n",
    "# Convert to different formats\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Analyze image quality\n",
    "quality_metrics = analyze_image_quality(img)\n",
    "\n",
    "print(f\"ðŸ“Š Image Quality Analysis:\")\n",
    "print(f\"Shape: {img.shape}\")\n",
    "print(f\"Brightness: {quality_metrics['brightness']:.1f} (0-255)\")\n",
    "print(f\"Contrast (std): {quality_metrics['contrast']:.1f}\")\n",
    "print(f\"Dynamic Range: {quality_metrics['dynamic_range']}\")\n",
    "print(f\"Noise Estimate: {quality_metrics['noise_estimate']:.1f}\")\n",
    "print(f\"Edge Density: {quality_metrics['edge_density']:.3f}\")\n",
    "\n",
    "# Show histogram\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(rgb_img)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(gray_img.ravel(), 256, [0, 256], alpha=0.7)\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Show color channel histograms\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, color in enumerate(colors):\n",
    "    hist = cv2.calcHist([img], [i], None, [256], [0, 256])\n",
    "    plt.plot(hist, color=color, alpha=0.7, label=f'{color.upper()} channel')\n",
    "plt.title('Color Channel Histograms')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store original for comparison\n",
    "original_img = img.copy()\n",
    "original_gray = gray_img.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c92f47f",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 2: Noise Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2.1 Different Denoising Techniques\n",
    "# ========================================\n",
    "\n",
    "# Gaussian blur for general noise\n",
    "gaussian_denoised = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "# Median filter for salt-and-pepper noise\n",
    "median_denoised = cv2.medianBlur(img, 5)\n",
    "\n",
    "# Bilateral filter for edge-preserving denoising\n",
    "bilateral_denoised = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "\n",
    "# Non-local means denoising (advanced)\n",
    "nlm_denoised = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 21)\n",
    "\n",
    "# Morphological opening for noise removal\n",
    "kernel = np.ones((3, 3), np.uint8)\n",
    "morphological_denoised = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "# ========================================\n",
    "# 2.2 Custom Wiener-like Filter\n",
    "# ========================================\n",
    "def wiener_filter(img, noise_variance=100):\n",
    "    \"\"\"Simple Wiener-like filter implementation\"\"\"\n",
    "    # Convert to frequency domain\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    f_transform = np.fft.fft2(img_gray)\n",
    "    f_shift = np.fft.fftshift(f_transform)\n",
    "    \n",
    "    # Create Wiener filter\n",
    "    rows, cols = img_gray.shape\n",
    "    power_spectrum = np.abs(f_shift) ** 2\n",
    "    wiener_filter = power_spectrum / (power_spectrum + noise_variance)\n",
    "    \n",
    "    # Apply filter\n",
    "    filtered = f_shift * wiener_filter\n",
    "    f_ishift = np.fft.ifftshift(filtered)\n",
    "    img_filtered = np.fft.ifft2(f_ishift)\n",
    "    img_filtered = np.abs(img_filtered)\n",
    "    \n",
    "    # Convert back to color\n",
    "    result = img.copy()\n",
    "    for i in range(3):\n",
    "        result[:, :, i] = cv2.normalize(img_filtered, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    return result.astype(np.uint8)\n",
    "\n",
    "wiener_denoised = wiener_filter(img)\n",
    "\n",
    "# Analyze noise reduction effectiveness\n",
    "print(\"ðŸ”§ Noise Reduction Analysis:\")\n",
    "print(f\"Original noise estimate: {analyze_image_quality(img)['noise_estimate']:.1f}\")\n",
    "print(f\"Gaussian denoised: {analyze_image_quality(gaussian_denoised)['noise_estimate']:.1f}\")\n",
    "print(f\"Median denoised: {analyze_image_quality(median_denoised)['noise_estimate']:.1f}\")\n",
    "print(f\"Bilateral denoised: {analyze_image_quality(bilateral_denoised)['noise_estimate']:.1f}\")\n",
    "print(f\"NLM denoised: {analyze_image_quality(nlm_denoised)['noise_estimate']:.1f}\")\n",
    "\n",
    "show_results(\n",
    "    [cv2.cvtColor(img, cv2.COLOR_BGR2RGB), \n",
    "     cv2.cvtColor(gaussian_denoised, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(median_denoised, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(bilateral_denoised, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(nlm_denoised, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(wiener_denoised, cv2.COLOR_BGR2RGB)],\n",
    "    ['Original', 'Gaussian', 'Median', 'Bilateral', 'NLM', 'Wiener-like'],\n",
    "    rows=2, cols=3\n",
    ")\n",
    "\n",
    "# Choose best denoising result\n",
    "denoised_img = bilateral_denoised  # Often provides good balance\n",
    "print(\"âœ… Selected bilateral filter for further processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139d00c",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 3: Contrast Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0af237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3.1 Global Contrast Enhancement\n",
    "# ========================================\n",
    "\n",
    "# Convert to grayscale for some operations\n",
    "denoised_gray = cv2.cvtColor(denoised_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Histogram Equalization\n",
    "hist_eq = cv2.equalizeHist(denoised_gray)\n",
    "\n",
    "# Contrast Stretching (Normalization)\n",
    "contrast_stretched = cv2.normalize(denoised_gray, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "# CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "clahe_enhanced = clahe.apply(denoised_gray)\n",
    "\n",
    "# ========================================\n",
    "# 3.2 Gamma Correction\n",
    "# ========================================\n",
    "def gamma_correction(img, gamma=1.2):\n",
    "    \"\"\"Apply gamma correction\"\"\"\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(img, table)\n",
    "\n",
    "gamma_corrected = gamma_correction(denoised_gray, gamma=0.8)  # Brighten\n",
    "\n",
    "# ========================================\n",
    "# 3.3 Local Contrast Enhancement\n",
    "# ========================================\n",
    "def local_contrast_enhancement(img, kernel_size=9):\n",
    "    \"\"\"Enhance local contrast using local statistics\"\"\"\n",
    "    # Calculate local mean and standard deviation\n",
    "    mean = cv2.blur(img.astype(np.float32), (kernel_size, kernel_size))\n",
    "    sqr_img = img.astype(np.float32) ** 2\n",
    "    sqr_mean = cv2.blur(sqr_img, (kernel_size, kernel_size))\n",
    "    std = np.sqrt(sqr_mean - mean ** 2)\n",
    "    \n",
    "    # Local contrast enhancement\n",
    "    enhanced = img.astype(np.float32)\n",
    "    mask = std > 0\n",
    "    enhanced[mask] = mean[mask] + 2 * (enhanced[mask] - mean[mask]) * (std[mask] / (std[mask] + 1))\n",
    "    \n",
    "    return np.clip(enhanced, 0, 255).astype(np.uint8)\n",
    "\n",
    "local_enhanced = local_contrast_enhancement(denoised_gray)\n",
    "\n",
    "# ========================================\n",
    "# 3.4 Adaptive Contrast Enhancement\n",
    "# ========================================\n",
    "def adaptive_contrast(img, alpha=1.5, beta=50):\n",
    "    \"\"\"Adaptive contrast enhancement\"\"\"\n",
    "    # Calculate local statistics\n",
    "    blurred = cv2.GaussianBlur(img.astype(np.float32), (15, 15), 0)\n",
    "    enhanced = alpha * (img.astype(np.float32) - blurred) + blurred + beta\n",
    "    return np.clip(enhanced, 0, 255).astype(np.uint8)\n",
    "\n",
    "adaptive_enhanced = adaptive_contrast(denoised_gray)\n",
    "\n",
    "# Analyze contrast improvement\n",
    "print(\"ðŸ“ˆ Contrast Enhancement Analysis:\")\n",
    "print(f\"Original contrast (std): {np.std(denoised_gray):.1f}\")\n",
    "print(f\"Histogram equalized: {np.std(hist_eq):.1f}\")\n",
    "print(f\"Contrast stretched: {np.std(contrast_stretched):.1f}\")\n",
    "print(f\"CLAHE enhanced: {np.std(clahe_enhanced):.1f}\")\n",
    "print(f\"Gamma corrected: {np.std(gamma_corrected):.1f}\")\n",
    "print(f\"Local enhanced: {np.std(local_enhanced):.1f}\")\n",
    "print(f\"Adaptive enhanced: {np.std(adaptive_enhanced):.1f}\")\n",
    "\n",
    "show_results(\n",
    "    [denoised_gray, hist_eq, contrast_stretched, clahe_enhanced, gamma_corrected, local_enhanced],\n",
    "    ['Denoised', 'Hist Equalized', 'Contrast Stretched', 'CLAHE', 'Gamma Corrected', 'Local Enhanced'],\n",
    "    rows=2, cols=3\n",
    ")\n",
    "\n",
    "# Choose best contrast enhancement\n",
    "contrast_enhanced = clahe_enhanced  # Often provides good results\n",
    "print(\"âœ… Selected CLAHE for contrast enhancement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7da477",
   "metadata": {},
   "source": [
    "## âš¡ Step 4: Sharpening and Detail Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4.1 Traditional Sharpening Kernels\n",
    "# ========================================\n",
    "\n",
    "# Basic sharpening kernel\n",
    "sharpen_kernel = np.array([[-1, -1, -1],\n",
    "                          [-1,  9, -1],\n",
    "                          [-1, -1, -1]])\n",
    "kernel_sharpened = cv2.filter2D(contrast_enhanced, -1, sharpen_kernel)\n",
    "\n",
    "# Laplacian sharpening\n",
    "laplacian = cv2.Laplacian(contrast_enhanced, cv2.CV_64F)\n",
    "laplacian_sharpened = np.clip(contrast_enhanced.astype(np.float64) - 0.3 * laplacian, 0, 255).astype(np.uint8)\n",
    "\n",
    "# ========================================\n",
    "# 4.2 Unsharp Masking\n",
    "# ========================================\n",
    "def unsharp_mask(img, blur_size=5, strength=1.5, threshold=0):\n",
    "    \"\"\"Apply unsharp masking for sharpening\"\"\"\n",
    "    # Create blurred version\n",
    "    blurred = cv2.GaussianBlur(img, (blur_size, blur_size), 0)\n",
    "    \n",
    "    # Calculate mask\n",
    "    mask = img.astype(np.float32) - blurred.astype(np.float32)\n",
    "    \n",
    "    # Apply threshold\n",
    "    if threshold > 0:\n",
    "        low_contrast_mask = np.absolute(mask) < threshold\n",
    "        mask[low_contrast_mask] = 0\n",
    "    \n",
    "    # Apply mask\n",
    "    sharpened = img.astype(np.float32) + strength * mask\n",
    "    \n",
    "    return np.clip(sharpened, 0, 255).astype(np.uint8)\n",
    "\n",
    "unsharp_sharpened = unsharp_mask(contrast_enhanced, blur_size=3, strength=1.2)\n",
    "\n",
    "# ========================================\n",
    "# 4.3 High-Boost Filtering\n",
    "# ========================================\n",
    "def high_boost_filter(img, amplification=1.5):\n",
    "    \"\"\"Apply high-boost filtering\"\"\"\n",
    "    # Low-pass filter\n",
    "    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    \n",
    "    # High-frequency component\n",
    "    high_freq = img.astype(np.float32) - blurred.astype(np.float32)\n",
    "    \n",
    "    # High-boost filter\n",
    "    boosted = img.astype(np.float32) + amplification * high_freq\n",
    "    \n",
    "    return np.clip(boosted, 0, 255).astype(np.uint8)\n",
    "\n",
    "high_boost_sharpened = high_boost_filter(contrast_enhanced, amplification=1.3)\n",
    "\n",
    "# ========================================\n",
    "# 4.4 Frequency Domain Sharpening\n",
    "# ========================================\n",
    "def frequency_domain_sharpen(img):\n",
    "    \"\"\"Sharpen using frequency domain high-pass filter\"\"\"\n",
    "    # FFT\n",
    "    f_transform = np.fft.fft2(img)\n",
    "    f_shift = np.fft.fftshift(f_transform)\n",
    "    \n",
    "    # Create high-pass filter\n",
    "    rows, cols = img.shape\n",
    "    crow, ccol = rows // 2, cols // 2\n",
    "    \n",
    "    # Butterworth high-pass filter\n",
    "    def butterworth_hp(d0, n=2):\n",
    "        r, c = np.mgrid[0:rows, 0:cols]\n",
    "        d = np.sqrt((r - crow)**2 + (c - ccol)**2)\n",
    "        return 1 / (1 + (d0 / (d + 1e-8))**(2*n))\n",
    "    \n",
    "    hp_filter = butterworth_hp(30)\n",
    "    \n",
    "    # Apply filter\n",
    "    filtered = f_shift * hp_filter\n",
    "    \n",
    "    # IFFT\n",
    "    f_ishift = np.fft.ifftshift(filtered)\n",
    "    img_filtered = np.fft.ifft2(f_ishift)\n",
    "    img_filtered = np.abs(img_filtered)\n",
    "    \n",
    "    # Combine with original\n",
    "    result = img.astype(np.float32) + 0.5 * img_filtered\n",
    "    \n",
    "    return np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "freq_sharpened = frequency_domain_sharpen(contrast_enhanced)\n",
    "\n",
    "# Analyze sharpening effectiveness\n",
    "def calculate_sharpness(img):\n",
    "    \"\"\"Calculate image sharpness using Laplacian variance\"\"\"\n",
    "    return cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "\n",
    "print(\"âš¡ Sharpening Analysis:\")\n",
    "print(f\"Original sharpness: {calculate_sharpness(contrast_enhanced):.1f}\")\n",
    "print(f\"Kernel sharpened: {calculate_sharpness(kernel_sharpened):.1f}\")\n",
    "print(f\"Laplacian sharpened: {calculate_sharpness(laplacian_sharpened):.1f}\")\n",
    "print(f\"Unsharp masked: {calculate_sharpness(unsharp_sharpened):.1f}\")\n",
    "print(f\"High-boost filtered: {calculate_sharpness(high_boost_sharpened):.1f}\")\n",
    "print(f\"Frequency sharpened: {calculate_sharpness(freq_sharpened):.1f}\")\n",
    "\n",
    "show_results(\n",
    "    [contrast_enhanced, kernel_sharpened, laplacian_sharpened, \n",
    "     unsharp_sharpened, high_boost_sharpened, freq_sharpened],\n",
    "    ['Contrast Enhanced', 'Kernel Sharpen', 'Laplacian Sharpen', \n",
    "     'Unsharp Mask', 'High-Boost', 'Frequency Sharpen'],\n",
    "    rows=2, cols=3\n",
    ")\n",
    "\n",
    "# Choose best sharpening result\n",
    "sharpened_img = unsharp_sharpened  # Often provides good balance\n",
    "print(\"âœ… Selected unsharp masking for detail enhancement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf18a99",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Step 5: Color Correction and White Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d693f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5.1 Apply enhancements to color image\n",
    "# ========================================\n",
    "\n",
    "# Apply the same processing pipeline to color image\n",
    "# Start with denoised color image\n",
    "enhanced_color = denoised_img.copy()\n",
    "\n",
    "# Apply CLAHE to each channel separately\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "lab = cv2.cvtColor(enhanced_color, cv2.COLOR_BGR2LAB)\n",
    "lab[:, :, 0] = clahe.apply(lab[:, :, 0])  # Apply to L channel only\n",
    "enhanced_color = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Apply unsharp masking to each channel\n",
    "for i in range(3):\n",
    "    enhanced_color[:, :, i] = unsharp_mask(enhanced_color[:, :, i], blur_size=3, strength=1.1)\n",
    "\n",
    "# ========================================\n",
    "# 5.2 White Balance Correction\n",
    "# ========================================\n",
    "\n",
    "def gray_world_white_balance(img):\n",
    "    \"\"\"Gray World white balancing\"\"\"\n",
    "    avg_b, avg_g, avg_r = np.mean(img, axis=(0, 1))\n",
    "    avg = (avg_b + avg_g + avg_r) / 3\n",
    "    \n",
    "    scale_b = avg / avg_b if avg_b > 0 else 1\n",
    "    scale_g = avg / avg_g if avg_g > 0 else 1\n",
    "    scale_r = avg / avg_r if avg_r > 0 else 1\n",
    "    \n",
    "    balanced = enhanced_color.astype(np.float32)\n",
    "    balanced[:, :, 0] *= scale_b\n",
    "    balanced[:, :, 1] *= scale_g\n",
    "    balanced[:, :, 2] *= scale_r\n",
    "    \n",
    "    return np.clip(balanced, 0, 255).astype(np.uint8)\n",
    "\n",
    "def white_patch_white_balance(img, percentile=99):\n",
    "    \"\"\"White Patch white balancing\"\"\"\n",
    "    max_b = np.percentile(img[:, :, 0], percentile)\n",
    "    max_g = np.percentile(img[:, :, 1], percentile)\n",
    "    max_r = np.percentile(img[:, :, 2], percentile)\n",
    "    \n",
    "    max_val = 250.0\n",
    "    scale_b = max_val / max_b if max_b > 0 else 1\n",
    "    scale_g = max_val / max_g if max_g > 0 else 1\n",
    "    scale_r = max_val / max_r if max_r > 0 else 1\n",
    "    \n",
    "    balanced = enhanced_color.astype(np.float32)\n",
    "    balanced[:, :, 0] = np.clip(balanced[:, :, 0] * scale_b, 0, 255)\n",
    "    balanced[:, :, 1] = np.clip(balanced[:, :, 1] * scale_g, 0, 255)\n",
    "    balanced[:, :, 2] = np.clip(balanced[:, :, 2] * scale_r, 0, 255)\n",
    "    \n",
    "    return balanced.astype(np.uint8)\n",
    "\n",
    "gray_world_balanced = gray_world_white_balance(enhanced_color)\n",
    "white_patch_balanced = white_patch_white_balance(enhanced_color)\n",
    "\n",
    "# ========================================\n",
    "# 5.3 Color Saturation Enhancement\n",
    "# ========================================\n",
    "def enhance_saturation(img, saturation_scale=1.3):\n",
    "    \"\"\"Enhance color saturation\"\"\"\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
    "    hsv[:, :, 1] *= saturation_scale  # Increase saturation\n",
    "    hsv[:, :, 1] = np.clip(hsv[:, :, 1], 0, 255)\n",
    "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
    "\n",
    "saturation_enhanced = enhance_saturation(gray_world_balanced)\n",
    "\n",
    "# ========================================\n",
    "# 5.4 Vibrance Enhancement (selective saturation)\n",
    "# ========================================\n",
    "def enhance_vibrance(img, vibrance=30):\n",
    "    \"\"\"Enhance vibrance (selective saturation boost)\"\"\"\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
    "    \n",
    "    # Get current saturation\n",
    "    s = hsv[:, :, 1] / 255.0\n",
    "    \n",
    "    # Calculate vibrance boost (less saturated pixels get more boost)\n",
    "    boost = vibrance * (1 - s) / 100.0\n",
    "    \n",
    "    # Apply boost\n",
    "    hsv[:, :, 1] = np.clip(hsv[:, :, 1] + boost * 255, 0, 255)\n",
    "    \n",
    "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
    "\n",
    "vibrance_enhanced = enhance_vibrance(gray_world_balanced)\n",
    "\n",
    "print(\"ðŸŽ¨ Color Analysis:\")\n",
    "def analyze_color_balance(img, name):\n",
    "    avg_b, avg_g, avg_r = np.mean(img, axis=(0, 1))\n",
    "    print(f\"{name}: B={avg_b:.1f}, G={avg_g:.1f}, R={avg_r:.1f}\")\n",
    "\n",
    "analyze_color_balance(enhanced_color, \"Enhanced\")\n",
    "analyze_color_balance(gray_world_balanced, \"Gray World\")\n",
    "analyze_color_balance(white_patch_balanced, \"White Patch\")\n",
    "\n",
    "show_results(\n",
    "    [cv2.cvtColor(enhanced_color, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(gray_world_balanced, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(white_patch_balanced, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(saturation_enhanced, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(vibrance_enhanced, cv2.COLOR_BGR2RGB)],\n",
    "    ['Enhanced Color', 'Gray World WB', 'White Patch WB', 'Saturation Enhanced', 'Vibrance Enhanced'],\n",
    "    rows=2, cols=3\n",
    ")\n",
    "\n",
    "# Choose final color correction\n",
    "final_color = vibrance_enhanced\n",
    "print(\"âœ… Selected vibrance enhancement for final color\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb63515",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 6: Final Optimization and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b002a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6.1 Final Refinements\n",
    "# ========================================\n",
    "\n",
    "# Apply a subtle final sharpening\n",
    "final_enhanced = final_color.copy()\n",
    "for i in range(3):\n",
    "    final_enhanced[:, :, i] = unsharp_mask(final_enhanced[:, :, i], blur_size=5, strength=0.3)\n",
    "\n",
    "# Subtle noise reduction on the final result\n",
    "final_enhanced = cv2.bilateralFilter(final_enhanced, 5, 10, 10)\n",
    "\n",
    "# ========================================\n",
    "# 6.2 Quality Assessment\n",
    "# ========================================\n",
    "print(\"ðŸŽ¯ Final Quality Assessment:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "original_quality = analyze_image_quality(original_img)\n",
    "final_quality = analyze_image_quality(final_enhanced)\n",
    "\n",
    "improvements = {\n",
    "    'brightness': final_quality['brightness'] - original_quality['brightness'],\n",
    "    'contrast': final_quality['contrast'] - original_quality['contrast'],\n",
    "    'dynamic_range': final_quality['dynamic_range'] - original_quality['dynamic_range'],\n",
    "    'noise_reduction': original_quality['noise_estimate'] - final_quality['noise_estimate'],\n",
    "    'edge_density': final_quality['edge_density'] - original_quality['edge_density']\n",
    "}\n",
    "\n",
    "print(\"Quality Metrics Comparison:\")\n",
    "print(f\"Brightness: {original_quality['brightness']:.1f} â†’ {final_quality['brightness']:.1f} ({improvements['brightness']:+.1f})\")\n",
    "print(f\"Contrast: {original_quality['contrast']:.1f} â†’ {final_quality['contrast']:.1f} ({improvements['contrast']:+.1f})\")\n",
    "print(f\"Dynamic Range: {original_quality['dynamic_range']} â†’ {final_quality['dynamic_range']} ({improvements['dynamic_range']:+.0f})\")\n",
    "print(f\"Noise Level: {original_quality['noise_estimate']:.1f} â†’ {final_quality['noise_estimate']:.1f} ({-improvements['noise_reduction']:+.1f})\")\n",
    "print(f\"Edge Density: {original_quality['edge_density']:.3f} â†’ {final_quality['edge_density']:.3f} ({improvements['edge_density']:+.3f})\")\n",
    "\n",
    "# ========================================\n",
    "# 6.3 Side-by-Side Comparison\n",
    "# ========================================\n",
    "comparison_imgs = [\n",
    "    cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB),\n",
    "    cv2.cvtColor(denoised_img, cv2.COLOR_BGR2RGB),\n",
    "    contrast_enhanced,\n",
    "    sharpened_img,\n",
    "    cv2.cvtColor(gray_world_balanced, cv2.COLOR_BGR2RGB),\n",
    "    cv2.cvtColor(final_enhanced, cv2.COLOR_BGR2RGB)\n",
    "]\n",
    "\n",
    "comparison_titles = [\n",
    "    'Original Image',\n",
    "    'Denoised',\n",
    "    'Contrast Enhanced',\n",
    "    'Sharpened',\n",
    "    'Color Corrected',\n",
    "    'Final Enhanced'\n",
    "]\n",
    "\n",
    "show_results(comparison_imgs, comparison_titles, rows=2, cols=3, figsize=(18, 12))\n",
    "\n",
    "# ========================================\n",
    "# 6.4 Enhancement Summary\n",
    "# ========================================\n",
    "print(f\"\\nðŸ“Š Enhancement Pipeline Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ… Processing Steps Applied:\")\n",
    "print(\"1. Noise Reduction - Bilateral filtering\")\n",
    "print(\"2. Contrast Enhancement - CLAHE\")\n",
    "print(\"3. Detail Enhancement - Unsharp masking\")\n",
    "print(\"4. Color Correction - Gray world white balance\")\n",
    "print(\"5. Saturation Enhancement - Vibrance boost\")\n",
    "print(\"6. Final Refinement - Subtle sharpening and denoising\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Key Improvements:\")\n",
    "if improvements['contrast'] > 0:\n",
    "    print(f\"âœ“ Contrast improved by {improvements['contrast']:.1f}\")\n",
    "if improvements['noise_reduction'] > 0:\n",
    "    print(f\"âœ“ Noise reduced by {improvements['noise_reduction']:.1f}\")\n",
    "if improvements['dynamic_range'] > 0:\n",
    "    print(f\"âœ“ Dynamic range increased by {improvements['dynamic_range']:.0f}\")\n",
    "if improvements['edge_density'] > 0:\n",
    "    print(f\"âœ“ Edge definition improved by {improvements['edge_density']:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Image Enhancement Completed Successfully!\")\n",
    "print(f\"ðŸ“š This pipeline demonstrates:\")\n",
    "print(f\"   - Comprehensive noise reduction techniques\")\n",
    "print(f\"   - Advanced contrast enhancement methods\")\n",
    "print(f\"   - Multiple sharpening approaches\")\n",
    "print(f\"   - Color correction and white balancing\")\n",
    "print(f\"   - Quality assessment and comparison\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
