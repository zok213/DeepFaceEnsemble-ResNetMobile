{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe5573e",
   "metadata": {},
   "source": [
    "# ðŸ“š CPV301 Complete Image Processing Pipeline\n",
    "## Master Pipeline for Practical Exam - All Topics Covered\n",
    "\n",
    "This notebook contains a comprehensive pipeline that processes an input image through ALL the topics covered in CPV301:\n",
    "\n",
    "### ðŸŽ¯ Pipeline Coverage:\n",
    "1. **Image Loading & Basic Operations**\n",
    "2. **Color Space Conversions** \n",
    "3. **Point Operators** (Histogram Equalization, Thresholding, Contrast Stretching)\n",
    "4. **Image Smoothing & Filtering**\n",
    "5. **Edge Detection** (Gradients, Canny)\n",
    "6. **Morphological Transformations**\n",
    "7. **Geometric Transformations**\n",
    "8. **White Balancing**\n",
    "9. **Feature Detection** (Harris, FAST, SIFT)\n",
    "10. **Fourier Transform**\n",
    "\n",
    "### ðŸ“ Instructions:\n",
    "- Replace the image path in the first code cell with your test image\n",
    "- Run all cells to see the complete processing pipeline\n",
    "- Each section shows theory, implementation, and results\n",
    "- Perfect for exam preparation and understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b4955",
   "metadata": {},
   "source": [
    "## ðŸš€ Essential Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd61a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for all image processing operations\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib parameters for better visualization\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "def show_images(images, titles, rows=2, cols=3, figsize=(15, 10), cmap_list=None):\n",
    "    \"\"\"\n",
    "    Enhanced function to display multiple images in a grid\n",
    "    \n",
    "    Args:\n",
    "        images: List of images to display\n",
    "        titles: List of titles for each image\n",
    "        rows, cols: Grid dimensions\n",
    "        figsize: Figure size\n",
    "        cmap_list: List of colormaps for each image (optional)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten() if rows * cols > 1 else [axes]\n",
    "    \n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Determine colormap\n",
    "        if cmap_list and i < len(cmap_list):\n",
    "            cmap = cmap_list[i]\n",
    "        elif len(img.shape) == 2:\n",
    "            cmap = 'gray'\n",
    "        else:\n",
    "            cmap = None\n",
    "            \n",
    "        # Handle BGR to RGB conversion for color images\n",
    "        if len(img.shape) == 3 and cmap is None:\n",
    "            display_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            display_img = img\n",
    "            \n",
    "        ax.imshow(display_img, cmap=cmap)\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide remaining subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_test_image():\n",
    "    \"\"\"Create a test image if no real image is available\"\"\"\n",
    "    # Create a synthetic test image with various features\n",
    "    img = np.zeros((400, 400, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Add colored rectangles\n",
    "    cv2.rectangle(img, (50, 50), (150, 150), (255, 100, 100), -1)\n",
    "    cv2.rectangle(img, (200, 50), (350, 150), (100, 255, 100), -1)\n",
    "    cv2.rectangle(img, (50, 200), (150, 350), (100, 100, 255), -1)\n",
    "    \n",
    "    # Add circles\n",
    "    cv2.circle(img, (275, 275), 50, (255, 255, 100), -1)\n",
    "    cv2.circle(img, (125, 275), 30, (255, 150, 200), -1)\n",
    "    \n",
    "    # Add some noise\n",
    "    noise = np.random.randint(0, 50, img.shape, dtype=np.uint8)\n",
    "    img = cv2.add(img, noise)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def debug_image(img, title=\"Debug Image\"):\n",
    "    \"\"\"Quick debugging function to check image properties\"\"\"\n",
    "    print(f\"=== {title} ===\")\n",
    "    print(f\"Shape: {img.shape}\")\n",
    "    print(f\"Type: {img.dtype}\")\n",
    "    print(f\"Min: {img.min()}, Max: {img.max()}\")\n",
    "    print(f\"Mean: {img.mean():.2f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"âœ… Helper functions loaded successfully!\")\n",
    "print(\"ðŸ“ Ready to process images through the complete pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c3536",
   "metadata": {},
   "source": [
    "## ðŸ“– 1. Image Loading and Basic Operations\n",
    "### Load your image here and perform basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa221a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CHANGE THIS PATH TO YOUR TEST IMAGE\n",
    "# ========================================\n",
    "IMAGE_PATH = 'D:/FPT_Material/Sem 4/CPV301/Source for PE/Image/image.jpg'\n",
    "\n",
    "# Try to load the specified image, fallback to test image if not found\n",
    "try:\n",
    "    original_img = cv2.imread(IMAGE_PATH)\n",
    "    if original_img is None:\n",
    "        raise FileNotFoundError(\"Image not found\")\n",
    "    print(f\"âœ… Successfully loaded image from: {IMAGE_PATH}\")\n",
    "except:\n",
    "    print(\"âš ï¸ Could not load specified image, creating test image...\")\n",
    "    original_img = create_test_image()\n",
    "\n",
    "# Get image properties\n",
    "debug_image(original_img, \"Original Image\")\n",
    "\n",
    "# Convert to different basic formats\n",
    "rgb_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "gray_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Display basic formats\n",
    "show_images(\n",
    "    [rgb_img, gray_img],\n",
    "    ['Original Image (RGB)', 'Grayscale Version'],\n",
    "    rows=1, cols=2, figsize=(12, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d6301",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ 2. Color Space Conversions\n",
    "### Converting between different color representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de1906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to various color spaces\n",
    "hsv_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2HSV)\n",
    "lab_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2LAB)\n",
    "ycrcb_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2YCrCb)\n",
    "luv_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2LUV)\n",
    "\n",
    "# For proper display, convert back to RGB\n",
    "hsv_display = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2RGB)\n",
    "lab_display = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "ycrcb_display = cv2.cvtColor(ycrcb_img, cv2.COLOR_YCrCb2RGB)\n",
    "luv_display = cv2.cvtColor(luv_img, cv2.COLOR_LUV2RGB)\n",
    "\n",
    "print(\"ðŸŽ¨ Color Space Analysis:\")\n",
    "print(f\"HSV range - H: [0,179], S: [0,255], V: [0,255]\")\n",
    "print(f\"LAB range - L: [0,100], A: [-127,127], B: [-127,127]\")\n",
    "print(f\"YCrCb range - Y: [16,235], Cr: [16,240], Cb: [16,240]\")\n",
    "\n",
    "show_images(\n",
    "    [rgb_img, hsv_display, lab_display, ycrcb_display, luv_display, gray_img],\n",
    "    ['Original RGB', 'HSV Color Space', 'LAB Color Space', 'YCrCb Color Space', 'LUV Color Space', 'Grayscale'],\n",
    "    rows=2, cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca96a29f",
   "metadata": {},
   "source": [
    "## ðŸ”§ 3. Point Operators\n",
    "### Pixel-wise transformations: Histogram Equalization, Thresholding, Contrast Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abef94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3.1 Histogram Equalization\n",
    "# ========================================\n",
    "print(\"ðŸ“Š Applying Histogram Equalization...\")\n",
    "equalized_img = cv2.equalizeHist(gray_img)\n",
    "\n",
    "# ========================================\n",
    "# 3.2 Contrast Stretching (Normalization)\n",
    "# ========================================\n",
    "print(\"ðŸ“ˆ Applying Contrast Stretching...\")\n",
    "stretched_img = cv2.normalize(gray_img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "# ========================================\n",
    "# 3.3 Alpha Compositing Example\n",
    "# ========================================\n",
    "print(\"ðŸŽ­ Creating Alpha Compositing Example...\")\n",
    "def alpha_compositing(foreground, background, alpha_matte):\n",
    "    \"\"\"Alpha compositing: C = Î±F + (1-Î±)B\"\"\"\n",
    "    alpha = alpha_matte.astype(float) / 255.0\n",
    "    if len(alpha.shape) == 2:\n",
    "        alpha = np.repeat(alpha[:, :, np.newaxis], 3, axis=2)\n",
    "    \n",
    "    foreground = foreground.astype(float)\n",
    "    background = background.astype(float)\n",
    "    composite = alpha * foreground + (1 - alpha) * background\n",
    "    return np.clip(composite, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Create simple alpha matte (circular gradient)\n",
    "h, w = gray_img.shape\n",
    "center_x, center_y = w//2, h//2\n",
    "Y, X = np.ogrid[:h, :w]\n",
    "distances = np.sqrt((X - center_x)**2 + (Y - center_y)**2)\n",
    "alpha_matte = np.clip(255 - distances * 2, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Create a simple background (solid color)\n",
    "background = np.full_like(original_img, [100, 150, 200])  # Light blue background\n",
    "composite_result = alpha_compositing(original_img, background, alpha_matte)\n",
    "\n",
    "# Display point operations results\n",
    "show_images(\n",
    "    [gray_img, equalized_img, stretched_img, alpha_matte, cv2.cvtColor(composite_result, cv2.COLOR_BGR2RGB)],\n",
    "    ['Original Grayscale', 'Histogram Equalized', 'Contrast Stretched', 'Alpha Matte', 'Alpha Compositing'],\n",
    "    rows=2, cols=3\n",
    ")\n",
    "\n",
    "# Show histograms\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(gray_img.ravel(), 256, [0, 256], color='blue', alpha=0.7)\n",
    "plt.title('Original Histogram')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(equalized_img.ravel(), 256, [0, 256], color='green', alpha=0.7)\n",
    "plt.title('Equalized Histogram')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(stretched_img.ravel(), 256, [0, 256], color='red', alpha=0.7)\n",
    "plt.title('Stretched Histogram')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0291f",
   "metadata": {},
   "source": [
    "## ðŸ”’ 4. Thresholding Operations\n",
    "### Binary image creation using different thresholding techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b4a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4.1 Simple Thresholding\n",
    "# ========================================\n",
    "print(\"ðŸ”’ Applying Different Thresholding Techniques...\")\n",
    "\n",
    "ret1, thresh_binary = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY)\n",
    "ret2, thresh_binary_inv = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "ret3, thresh_trunc = cv2.threshold(gray_img, 127, 255, cv2.THRESH_TRUNC)\n",
    "ret4, thresh_tozero = cv2.threshold(gray_img, 127, 255, cv2.THRESH_TOZERO)\n",
    "ret5, thresh_tozero_inv = cv2.threshold(gray_img, 127, 255, cv2.THRESH_TOZERO_INV)\n",
    "\n",
    "# ========================================\n",
    "# 4.2 Adaptive Thresholding\n",
    "# ========================================\n",
    "# Apply slight blur to reduce noise\n",
    "blurred_for_adaptive = cv2.medianBlur(gray_img, 5)\n",
    "\n",
    "adaptive_mean = cv2.adaptiveThreshold(blurred_for_adaptive, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "                                     cv2.THRESH_BINARY, 11, 2)\n",
    "adaptive_gaussian = cv2.adaptiveThreshold(blurred_for_adaptive, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                         cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# ========================================\n",
    "# 4.3 Otsu's Thresholding\n",
    "# ========================================\n",
    "ret_otsu, otsu_thresh = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Also try Otsu with Gaussian filtering\n",
    "gaussian_blur = cv2.GaussianBlur(gray_img, (5, 5), 0)\n",
    "ret_otsu_blur, otsu_thresh_blur = cv2.threshold(gaussian_blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "print(f\"ðŸ“Š Thresholding Results:\")\n",
    "print(f\"Simple threshold value: {ret1}\")\n",
    "print(f\"Otsu threshold value: {ret_otsu:.1f}\")\n",
    "print(f\"Otsu with blur threshold value: {ret_otsu_blur:.1f}\")\n",
    "\n",
    "# Display all thresholding results\n",
    "show_images(\n",
    "    [gray_img, thresh_binary, thresh_binary_inv, thresh_trunc, thresh_tozero, thresh_tozero_inv],\n",
    "    ['Original', 'Binary', 'Binary Inverted', 'Truncated', 'To Zero', 'To Zero Inverted'],\n",
    "    rows=2, cols=3\n",
    ")\n",
    "\n",
    "show_images(\n",
    "    [gray_img, adaptive_mean, adaptive_gaussian, otsu_thresh, otsu_thresh_blur],\n",
    "    ['Original', 'Adaptive Mean', 'Adaptive Gaussian', 'Otsu', 'Otsu + Blur'],\n",
    "    rows=2, cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579db26",
   "metadata": {},
   "source": [
    "## ðŸŒŠ 5. Image Smoothing and Filtering\n",
    "### Noise reduction and smoothing using different filter types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ed3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5.1 Linear Filters\n",
    "# ========================================\n",
    "print(\"ðŸŒŠ Applying Various Smoothing Filters...\")\n",
    "\n",
    "# Averaging filter\n",
    "avg_blur = cv2.blur(original_img, (9, 9))\n",
    "\n",
    "# Gaussian filter\n",
    "gaussian_blur = cv2.GaussianBlur(original_img, (9, 9), 0)\n",
    "\n",
    "# Box filter (normalized and unnormalized)\n",
    "box_filter_norm = cv2.boxFilter(original_img, -1, (9, 9), normalize=True)\n",
    "box_filter_unnorm = cv2.boxFilter(original_img, -1, (9, 9), normalize=False)\n",
    "\n",
    "# ========================================\n",
    "# 5.2 Non-linear Filters\n",
    "# ========================================\n",
    "# Median filter (excellent for salt-and-pepper noise)\n",
    "median_blur = cv2.medianBlur(original_img, 9)\n",
    "\n",
    "# Bilateral filter (edge-preserving)\n",
    "bilateral_filter = cv2.bilateralFilter(original_img, 9, 75, 75)\n",
    "\n",
    "# ========================================\n",
    "# 5.3 Custom Kernel Filtering\n",
    "# ========================================\n",
    "# Create custom sharpening kernel\n",
    "sharpen_kernel = np.array([[-1, -1, -1],\n",
    "                          [-1,  9, -1],\n",
    "                          [-1, -1, -1]])\n",
    "sharpened = cv2.filter2D(original_img, -1, sharpen_kernel)\n",
    "\n",
    "print(\"ðŸ“Š Filter Comparison:\")\n",
    "print(\"- Averaging: Simple blur, may create artifacts\")\n",
    "print(\"- Gaussian: Smooth blur, good for noise reduction\")\n",
    "print(\"- Median: Excellent for salt-and-pepper noise\")\n",
    "print(\"- Bilateral: Preserves edges while smoothing\")\n",
    "\n",
    "show_images(\n",
    "    [cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB), \n",
    "     cv2.cvtColor(avg_blur, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(gaussian_blur, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(median_blur, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(bilateral_filter, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(sharpened, cv2.COLOR_BGR2RGB)],\n",
    "    ['Original', 'Average Blur', 'Gaussian Blur', 'Median Blur', 'Bilateral Filter', 'Sharpened'],\n",
    "    rows=2, cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d72d0",
   "metadata": {},
   "source": [
    "## âš¡ 6. Edge Detection and Gradients\n",
    "### Finding edges using gradient operators and Canny edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68614ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6.1 Gradient Operators\n",
    "# ========================================\n",
    "print(\"âš¡ Applying Gradient Operators and Edge Detection...\")\n",
    "\n",
    "# IMPORTANT: Always use CV_64F for gradients to capture negative values!\n",
    "sobel_x = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=5)\n",
    "sobel_y = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=5)\n",
    "\n",
    "# Scharr operator (more accurate than 3x3 Sobel)\n",
    "scharr_x = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=-1)  # ksize=-1 for Scharr\n",
    "scharr_y = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=-1)\n",
    "\n",
    "# Laplacian operator (second derivative)\n",
    "laplacian = cv2.Laplacian(gray_img, cv2.CV_64F)\n",
    "\n",
    "# Convert back to uint8 for display\n",
    "sobel_x_8u = np.uint8(np.absolute(sobel_x))\n",
    "sobel_y_8u = np.uint8(np.absolute(sobel_y))\n",
    "scharr_x_8u = np.uint8(np.absolute(scharr_x))\n",
    "scharr_y_8u = np.uint8(np.absolute(scharr_y))\n",
    "laplacian_8u = np.uint8(np.absolute(laplacian))\n",
    "\n",
    "# Combine gradients\n",
    "sobel_combined = np.uint8(np.sqrt(sobel_x**2 + sobel_y**2))\n",
    "scharr_combined = np.uint8(np.sqrt(scharr_x**2 + scharr_y**2))\n",
    "\n",
    "# ========================================\n",
    "# 6.2 Canny Edge Detection\n",
    "# ========================================\n",
    "# Preprocess with Gaussian blur\n",
    "preprocessed = cv2.GaussianBlur(gray_img, (5, 5), 0)\n",
    "\n",
    "# Apply Canny with different threshold combinations\n",
    "canny_low = cv2.Canny(preprocessed, 50, 100)\n",
    "canny_medium = cv2.Canny(preprocessed, 100, 200)\n",
    "canny_high = cv2.Canny(preprocessed, 150, 300)\n",
    "\n",
    "print(\"ðŸ“Š Edge Detection Analysis:\")\n",
    "print(f\"Sobel X gradient range: [{sobel_x.min():.1f}, {sobel_x.max():.1f}]\")\n",
    "print(f\"Sobel Y gradient range: [{sobel_y.min():.1f}, {sobel_y.max():.1f}]\")\n",
    "print(f\"Laplacian range: [{laplacian.min():.1f}, {laplacian.max():.1f}]\")\n",
    "\n",
    "# Display gradient results\n",
    "show_images(\n",
    "    [gray_img, sobel_x_8u, sobel_y_8u, sobel_combined, scharr_combined, laplacian_8u],\n",
    "    ['Original', 'Sobel X', 'Sobel Y', 'Sobel Combined', 'Scharr Combined', 'Laplacian'],\n",
    "    rows=2, cols=3\n",
    ")\n",
    "\n",
    "# Display Canny results\n",
    "show_images(\n",
    "    [gray_img, canny_low, canny_medium, canny_high],\n",
    "    ['Original', 'Canny (50,100)', 'Canny (100,200)', 'Canny (150,300)'],\n",
    "    rows=2, cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61092a",
   "metadata": {},
   "source": [
    "## ðŸ”§ 7. Morphological Transformations\n",
    "### Shape-based operations for binary image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 7.1 Basic Morphological Operations\n",
    "# ========================================\n",
    "print(\"ðŸ”§ Applying Morphological Transformations...\")\n",
    "\n",
    "# Use binary image from thresholding\n",
    "binary_img = thresh_binary.copy()\n",
    "\n",
    "# Define different kernels\n",
    "kernel_3x3 = np.ones((3, 3), np.uint8)\n",
    "kernel_5x5 = np.ones((5, 5), np.uint8)\n",
    "kernel_ellipse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "kernel_cross = cv2.getStructuringElement(cv2.MORPH_CROSS, (5, 5))\n",
    "\n",
    "# Basic operations\n",
    "erosion = cv2.erode(binary_img, kernel_5x5, iterations=1)\n",
    "dilation = cv2.dilate(binary_img, kernel_5x5, iterations=1)\n",
    "\n",
    "# Compound operations\n",
    "opening = cv2.morphologyEx(binary_img, cv2.MORPH_OPEN, kernel_5x5)\n",
    "closing = cv2.morphologyEx(binary_img, cv2.MORPH_CLOSE, kernel_5x5)\n",
    "morph_gradient = cv2.morphologyEx(binary_img, cv2.MORPH_GRADIENT, kernel_5x5)\n",
    "tophat = cv2.morphologyEx(binary_img, cv2.MORPH_TOPHAT, kernel_5x5)\n",
    "blackhat = cv2.morphologyEx(binary_img, cv2.MORPH_BLACKHAT, kernel_5x5)\n",
    "\n",
    "print(\"ðŸ“Š Morphological Operations Effects:\")\n",
    "print(\"- Erosion: Shrinks foreground objects\")\n",
    "print(\"- Dilation: Expands foreground objects\")\n",
    "print(\"- Opening: Removes small objects, separates connected objects\")\n",
    "print(\"- Closing: Fills small holes, connects nearby objects\")\n",
    "print(\"- Gradient: Highlights boundaries\")\n",
    "print(\"- Top Hat: Enhances bright details on dark background\")\n",
    "print(\"- Black Hat: Enhances dark details on bright background\")\n",
    "\n",
    "show_images(\n",
    "    [binary_img, erosion, dilation, opening, closing, morph_gradient],\n",
    "    ['Original Binary', 'Erosion', 'Dilation', 'Opening', 'Closing', 'Gradient'],\n",
    "    rows=2, cols=3\n",
    ")\n",
    "\n",
    "show_images(\n",
    "    [binary_img, tophat, blackhat],\n",
    "    ['Original Binary', 'Top Hat', 'Black Hat'],\n",
    "    rows=1, cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253a827",
   "metadata": {},
   "source": [
    "## ðŸ”„ 8. Geometric Transformations\n",
    "### Spatial transformations: Translation, Rotation, Scaling, Affine, Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ddafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 8.1 Basic Geometric Transformations\n",
    "# ========================================\n",
    "print(\"ðŸ”„ Applying Geometric Transformations...\")\n",
    "\n",
    "rows, cols = gray_img.shape\n",
    "center = (cols // 2, rows // 2)\n",
    "\n",
    "# ========================================\n",
    "# Translation\n",
    "# ========================================\n",
    "tx, ty = 50, 30  # Translation offsets\n",
    "M_translate = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "translated = cv2.warpAffine(gray_img, M_translate, (cols, rows))\n",
    "\n",
    "# ========================================\n",
    "# Rotation (Euclidean)\n",
    "# ========================================\n",
    "angle = 45  # degrees\n",
    "scale = 1.0\n",
    "M_rotate = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "rotated = cv2.warpAffine(gray_img, M_rotate, (cols, rows))\n",
    "\n",
    "# ========================================\n",
    "# Similarity Transformation (Rotation + Scaling)\n",
    "# ========================================\n",
    "angle_sim = 30\n",
    "scale_sim = 0.8\n",
    "M_similarity = cv2.getRotationMatrix2D(center, angle_sim, scale_sim)\n",
    "similarity = cv2.warpAffine(gray_img, M_similarity, (cols, rows))\n",
    "\n",
    "# ========================================\n",
    "# Affine Transformation\n",
    "# ========================================\n",
    "# Define 3 point correspondences\n",
    "pts1 = np.float32([[50, 50], [200, 50], [50, 200]])\n",
    "pts2 = np.float32([[10, 100], [200, 50], [100, 250]])\n",
    "\n",
    "# Scale points if image is too small\n",
    "if rows < 300 or cols < 300:\n",
    "    scale_factor = min(rows, cols) / 300\n",
    "    pts1 = pts1 * scale_factor\n",
    "    pts2 = pts2 * scale_factor\n",
    "\n",
    "M_affine = cv2.getAffineTransform(pts1, pts2)\n",
    "affine = cv2.warpAffine(gray_img, M_affine, (cols, rows))\n",
    "\n",
    "print(\"ðŸ“Š Transformation Hierarchy:\")\n",
    "print(\"Translation (2 DOF) â†’ Euclidean (3 DOF) â†’ Similarity (4 DOF) â†’ Affine (6 DOF) â†’ Perspective (8 DOF)\")\n",
    "print(f\"Original image size: {rows}x{cols}\")\n",
    "print(f\"Translation offset: ({tx}, {ty})\")\n",
    "print(f\"Rotation angle: {angle}Â°\")\n",
    "print(f\"Similarity: {angle_sim}Â° rotation, {scale_sim}x scale\")\n",
    "\n",
    "show_images(\n",
    "    [gray_img, translated, rotated, similarity, affine],\n",
    "    ['Original', 'Translated', 'Rotated', 'Similarity', 'Affine'],\n",
    "    rows=2, cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62d33c",
   "metadata": {},
   "source": [
    "## âš–ï¸ 9. White Balancing\n",
    "### Color correction algorithms for proper color representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 9.1 Gray World White Balancing\n",
    "# ========================================\n",
    "print(\"âš–ï¸ Applying White Balancing Algorithms...\")\n",
    "\n",
    "def gray_world_white_balance(img):\n",
    "    \"\"\"Gray World Algorithm: Assumes the average color should be gray\"\"\"\n",
    "    # Calculate channel averages\n",
    "    avg_b = np.mean(img[:, :, 0])\n",
    "    avg_g = np.mean(img[:, :, 1]) \n",
    "    avg_r = np.mean(img[:, :, 2])\n",
    "    \n",
    "    # Overall average\n",
    "    avg = (avg_b + avg_g + avg_r) / 3\n",
    "    \n",
    "    # Scaling factors\n",
    "    scale_b = avg / avg_b if avg_b > 0 else 1\n",
    "    scale_g = avg / avg_g if avg_g > 0 else 1\n",
    "    scale_r = avg / avg_r if avg_r > 0 else 1\n",
    "    \n",
    "    # Apply scaling\n",
    "    balanced = np.zeros_like(img, dtype=np.float32)\n",
    "    balanced[:, :, 0] = img[:, :, 0] * scale_b\n",
    "    balanced[:, :, 1] = img[:, :, 1] * scale_g\n",
    "    balanced[:, :, 2] = img[:, :, 2] * scale_r\n",
    "    \n",
    "    return np.clip(balanced, 0, 255).astype(np.uint8)\n",
    "\n",
    "def white_patch_white_balance(img, percentile=99):\n",
    "    \"\"\"White Patch Algorithm: Assumes brightest pixels should be white\"\"\"\n",
    "    # Find percentile values in each channel\n",
    "    max_b = np.percentile(img[:, :, 0], percentile)\n",
    "    max_g = np.percentile(img[:, :, 1], percentile)\n",
    "    max_r = np.percentile(img[:, :, 2], percentile)\n",
    "    \n",
    "    # Target value\n",
    "    max_val = 250.0\n",
    "    \n",
    "    # Scaling factors\n",
    "    scale_b = max_val / max_b if max_b > 0 else 1\n",
    "    scale_g = max_val / max_g if max_g > 0 else 1\n",
    "    scale_r = max_val / max_r if max_r > 0 else 1\n",
    "    \n",
    "    # Apply scaling\n",
    "    balanced = np.zeros_like(img, dtype=np.float32)\n",
    "    balanced[:, :, 0] = np.clip(img[:, :, 0] * scale_b, 0, 255)\n",
    "    balanced[:, :, 1] = np.clip(img[:, :, 1] * scale_g, 0, 255)\n",
    "    balanced[:, :, 2] = np.clip(img[:, :, 2] * scale_r, 0, 255)\n",
    "    \n",
    "    return balanced.astype(np.uint8)\n",
    "\n",
    "# Apply different white balancing algorithms\n",
    "gray_world_result = gray_world_white_balance(original_img)\n",
    "white_patch_result = white_patch_white_balance(original_img)\n",
    "\n",
    "# Analyze color statistics\n",
    "def analyze_color_stats(img, name):\n",
    "    avg_b, avg_g, avg_r = np.mean(img, axis=(0,1))\n",
    "    print(f\"{name}: B={avg_b:.1f}, G={avg_g:.1f}, R={avg_r:.1f}\")\n",
    "\n",
    "print(\"ðŸ“Š Color Statistics Analysis:\")\n",
    "analyze_color_stats(original_img, \"Original\")\n",
    "analyze_color_stats(gray_world_result, \"Gray World\")\n",
    "analyze_color_stats(white_patch_result, \"White Patch\")\n",
    "\n",
    "show_images(\n",
    "    [cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(gray_world_result, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(white_patch_result, cv2.COLOR_BGR2RGB)],\n",
    "    ['Original Image', 'Gray World WB', 'White Patch WB'],\n",
    "    rows=1, cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1356a0",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 10. Feature Detection\n",
    "### Detecting and describing key points: Harris, FAST, SIFT, ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f19ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 10.1 Harris Corner Detection\n",
    "# ========================================\n",
    "print(\"ðŸŽ¯ Applying Feature Detection Algorithms...\")\n",
    "\n",
    "# Harris corner detection\n",
    "gray_float = np.float32(gray_img)\n",
    "harris_corners = cv2.cornerHarris(gray_float, 2, 3, 0.04)\n",
    "\n",
    "# Create visualization\n",
    "harris_img = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR)\n",
    "harris_img[harris_corners > 0.01 * harris_corners.max()] = [0, 0, 255]  # Mark corners in red\n",
    "\n",
    "# ========================================\n",
    "# 10.2 FAST Corner Detection\n",
    "# ========================================\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "fast_keypoints = fast.detect(gray_img, None)\n",
    "fast_img = cv2.drawKeypoints(cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR), fast_keypoints, None, color=(0,255,0))\n",
    "\n",
    "# ========================================\n",
    "# 10.3 SIFT Features (if available)\n",
    "# ========================================\n",
    "try:\n",
    "    sift = cv2.SIFT_create()\n",
    "    sift_keypoints, sift_descriptors = sift.detectAndCompute(gray_img, None)\n",
    "    sift_img = cv2.drawKeypoints(cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR), sift_keypoints, None)\n",
    "    sift_available = True\n",
    "except:\n",
    "    print(\"âš ï¸ SIFT not available, using ORB instead...\")\n",
    "    sift_img = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR)\n",
    "    sift_keypoints = []\n",
    "    sift_available = False\n",
    "\n",
    "# ========================================\n",
    "# 10.4 ORB Features (Alternative to SIFT/SURF)\n",
    "# ========================================\n",
    "orb = cv2.ORB_create()\n",
    "orb_keypoints, orb_descriptors = orb.detectAndCompute(gray_img, None)\n",
    "orb_img = cv2.drawKeypoints(cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR), orb_keypoints, None, color=(255,0,0))\n",
    "\n",
    "# ========================================\n",
    "# 10.5 Good Features to Track\n",
    "# ========================================\n",
    "corners = cv2.goodFeaturesToTrack(gray_img, 100, 0.01, 10)\n",
    "corners_img = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "if corners is not None:\n",
    "    corners = np.int0(corners)\n",
    "    for corner in corners:\n",
    "        x, y = corner.ravel()\n",
    "        cv2.circle(corners_img, (x, y), 3, (255, 255, 0), -1)\n",
    "\n",
    "print(f\"ðŸ“Š Feature Detection Results:\")\n",
    "print(f\"Harris corners found: {np.sum(harris_corners > 0.01 * harris_corners.max())}\")\n",
    "print(f\"FAST keypoints: {len(fast_keypoints)}\")\n",
    "if sift_available:\n",
    "    print(f\"SIFT keypoints: {len(sift_keypoints)}\")\n",
    "print(f\"ORB keypoints: {len(orb_keypoints)}\")\n",
    "print(f\"Good features to track: {len(corners) if corners is not None else 0}\")\n",
    "\n",
    "show_images(\n",
    "    [cv2.cvtColor(harris_img, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(fast_img, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(sift_img, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(orb_img, cv2.COLOR_BGR2RGB),\n",
    "     cv2.cvtColor(corners_img, cv2.COLOR_BGR2RGB)],\n",
    "    ['Harris Corners', 'FAST Features', 'SIFT Features', 'ORB Features', 'Good Features to Track'],\n",
    "    rows=2, cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa2624",
   "metadata": {},
   "source": [
    "## ðŸŒŠ 11. Fourier Transform\n",
    "### Frequency domain analysis and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 11.1 Forward Fourier Transform\n",
    "# ========================================\n",
    "print(\"ðŸŒŠ Applying Fourier Transform Analysis...\")\n",
    "\n",
    "# Apply FFT\n",
    "f_transform = np.fft.fft2(gray_img)\n",
    "f_shift = np.fft.fftshift(f_transform)\n",
    "\n",
    "# Calculate magnitude spectrum for visualization\n",
    "magnitude_spectrum = 20 * np.log(np.abs(f_shift) + 1)  # +1 to avoid log(0)\n",
    "\n",
    "# ========================================\n",
    "# 11.2 Frequency Domain Filtering\n",
    "# ========================================\n",
    "rows, cols = gray_img.shape\n",
    "crow, ccol = rows // 2, cols // 2\n",
    "\n",
    "# Low-pass filter (removes high frequencies)\n",
    "mask_low = np.zeros((rows, cols), np.uint8)\n",
    "cv2.circle(mask_low, (ccol, crow), 50, 1, -1)\n",
    "\n",
    "# Apply low-pass filter\n",
    "fshift_low = f_shift * mask_low\n",
    "f_ishift_low = np.fft.ifftshift(fshift_low)\n",
    "img_back_low = np.fft.ifft2(f_ishift_low)\n",
    "img_back_low = np.abs(img_back_low)\n",
    "\n",
    "# High-pass filter (removes low frequencies)\n",
    "mask_high = np.ones((rows, cols), np.uint8)\n",
    "cv2.circle(mask_high, (ccol, crow), 50, 0, -1)\n",
    "\n",
    "# Apply high-pass filter\n",
    "fshift_high = f_shift * mask_high\n",
    "f_ishift_high = np.fft.ifftshift(fshift_high)\n",
    "img_back_high = np.fft.ifft2(f_ishift_high)\n",
    "img_back_high = np.abs(img_back_high)\n",
    "\n",
    "print(\"ðŸ“Š Frequency Domain Analysis:\")\n",
    "print(f\"Image size: {rows}x{cols}\")\n",
    "print(f\"Center frequency: ({crow}, {ccol})\")\n",
    "print(f\"Low-pass cutoff radius: 50 pixels\")\n",
    "print(f\"High-pass cutoff radius: 50 pixels\")\n",
    "\n",
    "# Display frequency domain results\n",
    "show_images(\n",
    "    [gray_img, magnitude_spectrum, mask_low*255, img_back_low.astype(np.uint8), \n",
    "     mask_high*255, img_back_high.astype(np.uint8)],\n",
    "    ['Original', 'Magnitude Spectrum', 'Low-pass Mask', 'Low-pass Result', \n",
    "     'High-pass Mask', 'High-pass Result'],\n",
    "    rows=2, cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc8b93",
   "metadata": {},
   "source": [
    "## ðŸ“Š 12. Pipeline Summary and Analysis\n",
    "### Final comparison and exam preparation summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 12.1 Pipeline Summary\n",
    "# ========================================\n",
    "print(\"ðŸ“Š Complete Pipeline Analysis Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Collect key results for final comparison\n",
    "final_results = [\n",
    "    ('Original Image', cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)),\n",
    "    ('Grayscale', gray_img),\n",
    "    ('Histogram Equalized', equalized_img),\n",
    "    ('Gaussian Blur', cv2.cvtColor(gaussian_blur, cv2.COLOR_BGR2RGB)),\n",
    "    ('Canny Edges', canny_medium),\n",
    "    ('Binary Threshold', thresh_binary),\n",
    "    ('Morphological Opening', opening),\n",
    "    ('Harris Corners', cv2.cvtColor(harris_img, cv2.COLOR_BGR2RGB)),\n",
    "    ('ORB Features', cv2.cvtColor(orb_img, cv2.COLOR_BGR2RGB)),\n",
    "    ('Rotated', rotated),\n",
    "    ('White Balanced', cv2.cvtColor(gray_world_result, cv2.COLOR_BGR2RGB)),\n",
    "    ('Frequency Filtered', img_back_low.astype(np.uint8))\n",
    "]\n",
    "\n",
    "# Create final comparison grid\n",
    "show_images(\n",
    "    [result[1] for result in final_results],\n",
    "    [result[0] for result in final_results],\n",
    "    rows=3, cols=4, figsize=(20, 15)\n",
    ")\n",
    "\n",
    "# ========================================\n",
    "# 12.2 Exam Tips Summary\n",
    "# ========================================\n",
    "print(f\"\\nðŸŽ¯ CPV301 Practical Exam Tips:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"1. Always check image loading: assert img is not None\")\n",
    "print(\"2. Use CV_64F for gradient operations to capture negative values\")\n",
    "print(\"3. Convert BGRâ†’RGB for matplotlib display\")\n",
    "print(\"4. Choose appropriate kernel sizes (odd numbers)\")\n",
    "print(\"5. Apply preprocessing (blur) before edge detection\")\n",
    "print(\"6. Use adaptive thresholding for non-uniform lighting\")\n",
    "print(\"7. Combine morphological operations for better results\")\n",
    "print(\"8. Test different parameter values and compare results\")\n",
    "print(\"9. Understand when to use each technique based on image characteristics\")\n",
    "print(\"10. Always validate results visually and quantitatively\")\n",
    "\n",
    "print(f\"\\nâœ… Pipeline completed successfully!\")\n",
    "print(f\"ðŸ“š All CPV301 topics covered and demonstrated!\")\n",
    "print(f\"ðŸ’ª You're ready for the practical exam!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
