{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76547a7c",
   "metadata": {},
   "source": [
    "# CPV301 Practical Exam - Complete Practice Notebook\n",
    "\n",
    "This notebook contains all the essential code patterns and examples you need for the practical exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a58276f",
   "metadata": {},
   "source": [
    "## Essential Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77439e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(images, titles, figsize=(15, 5)):\n",
    "    \"\"\"Helper function to display multiple images\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        plt.subplot(1, len(images), i+1)\n",
    "        if len(img.shape) == 3:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            plt.imshow(img, cmap='gray')\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create sample image for testing\n",
    "# You can replace this with cv2.imread('your_image.jpg') for real images\n",
    "sample_img = np.random.randint(0, 255, (300, 300, 3), dtype=np.uint8)\n",
    "sample_gray = cv2.cvtColor(sample_img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3efa2d",
   "metadata": {},
   "source": [
    "## 1. Point Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram Equalization\n",
    "equalized = cv2.equalizeHist(sample_gray)\n",
    "\n",
    "# Contrast Stretching\n",
    "stretched = cv2.normalize(sample_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Thresholding\n",
    "ret, binary = cv2.threshold(sample_gray, 127, 255, cv2.THRESH_BINARY)\n",
    "adaptive = cv2.adaptiveThreshold(sample_gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "ret_otsu, otsu = cv2.threshold(sample_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "show_images([sample_gray, equalized, stretched, binary, adaptive, otsu], \n",
    "           ['Original', 'Equalized', 'Stretched', 'Binary', 'Adaptive', 'Otsu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d44fe1",
   "metadata": {},
   "source": [
    "## 2. Image Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different smoothing filters\n",
    "blur = cv2.blur(sample_img, (5,5))\n",
    "gaussian = cv2.GaussianBlur(sample_img, (5,5), 0)\n",
    "median = cv2.medianBlur(sample_img, 5)\n",
    "bilateral = cv2.bilateralFilter(sample_img, 9, 75, 75)\n",
    "\n",
    "show_images([sample_img, blur, gaussian, median, bilateral],\n",
    "           ['Original', 'Averaging', 'Gaussian', 'Median', 'Bilateral'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3249c8",
   "metadata": {},
   "source": [
    "## 3. Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient operators - ALWAYS use CV_64F!\n",
    "sobel_x = cv2.Sobel(sample_gray, cv2.CV_64F, 1, 0, ksize=5)\n",
    "sobel_y = cv2.Sobel(sample_gray, cv2.CV_64F, 0, 1, ksize=5)\n",
    "laplacian = cv2.Laplacian(sample_gray, cv2.CV_64F)\n",
    "\n",
    "# Convert back to uint8\n",
    "sobel_x_8u = np.uint8(np.absolute(sobel_x))\n",
    "sobel_y_8u = np.uint8(np.absolute(sobel_y))\n",
    "laplacian_8u = np.uint8(np.absolute(laplacian))\n",
    "\n",
    "# Canny edge detection\n",
    "blurred = cv2.GaussianBlur(sample_gray, (5,5), 0)\n",
    "canny = cv2.Canny(blurred, 100, 200)\n",
    "\n",
    "show_images([sample_gray, sobel_x_8u, sobel_y_8u, laplacian_8u, canny],\n",
    "           ['Original', 'Sobel X', 'Sobel Y', 'Laplacian', 'Canny'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2460d8",
   "metadata": {},
   "source": [
    "## 4. Morphological Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78781dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use binary image for morphological operations\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "\n",
    "erosion = cv2.erode(binary, kernel, iterations=1)\n",
    "dilation = cv2.dilate(binary, kernel, iterations=1)\n",
    "opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "closing = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "gradient = cv2.morphologyEx(binary, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "show_images([binary, erosion, dilation, opening, closing, gradient],\n",
    "           ['Binary', 'Erosion', 'Dilation', 'Opening', 'Closing', 'Gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa35f9",
   "metadata": {},
   "source": [
    "## 5. Geometric Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74501e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = sample_gray.shape\n",
    "\n",
    "# Translation\n",
    "M_translate = np.float32([[1, 0, 50], [0, 1, 30]])\n",
    "translated = cv2.warpAffine(sample_gray, M_translate, (cols, rows))\n",
    "\n",
    "# Rotation\n",
    "center = (cols//2, rows//2)\n",
    "M_rotate = cv2.getRotationMatrix2D(center, 45, 1.0)\n",
    "rotated = cv2.warpAffine(sample_gray, M_rotate, (cols, rows))\n",
    "\n",
    "# Affine transformation\n",
    "pts1 = np.float32([[50,50], [200,50], [50,200]])\n",
    "pts2 = np.float32([[10,100], [200,50], [100,250]])\n",
    "M_affine = cv2.getAffineTransform(pts1, pts2)\n",
    "affine = cv2.warpAffine(sample_gray, M_affine, (cols, rows))\n",
    "\n",
    "show_images([sample_gray, translated, rotated, affine],\n",
    "           ['Original', 'Translated', 'Rotated', 'Affine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4933af",
   "metadata": {},
   "source": [
    "## 6. Color Space Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to different color spaces\n",
    "rgb_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)\n",
    "hsv_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2HSV)\n",
    "lab_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2LAB)\n",
    "ycrcb_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "# Display in RGB format for proper colors\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1), plt.imshow(rgb_img), plt.title('RGB'), plt.axis('off')\n",
    "plt.subplot(2, 2, 2), plt.imshow(cv2.cvtColor(hsv_img, cv2.COLOR_HSV2RGB)), plt.title('HSV'), plt.axis('off')\n",
    "plt.subplot(2, 2, 3), plt.imshow(cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)), plt.title('LAB'), plt.axis('off')\n",
    "plt.subplot(2, 2, 4), plt.imshow(cv2.cvtColor(ycrcb_img, cv2.COLOR_YCrCb2RGB)), plt.title('YCrCb'), plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219062c3",
   "metadata": {},
   "source": [
    "## 7. Feature Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harris Corner Detection\n",
    "gray_f32 = np.float32(sample_gray)\n",
    "harris_corners = cv2.cornerHarris(gray_f32, 2, 3, 0.04)\n",
    "harris_img = sample_img.copy()\n",
    "harris_img[harris_corners > 0.01 * harris_corners.max()] = [0, 0, 255]\n",
    "\n",
    "# FAST Corner Detection\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "fast_keypoints = fast.detect(sample_gray, None)\n",
    "fast_img = cv2.drawKeypoints(sample_img, fast_keypoints, None, color=(255,0,0))\n",
    "\n",
    "# SIFT Features\n",
    "sift = cv2.SIFT_create()\n",
    "sift_keypoints, sift_descriptors = sift.detectAndCompute(sample_gray, None)\n",
    "sift_img = cv2.drawKeypoints(sample_img, sift_keypoints, None)\n",
    "\n",
    "show_images([sample_img, harris_img, fast_img, sift_img],\n",
    "           ['Original', 'Harris Corners', 'FAST Features', 'SIFT Features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d17b5c",
   "metadata": {},
   "source": [
    "## 8. Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceed4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT\n",
    "f_transform = np.fft.fft2(sample_gray)\n",
    "f_shift = np.fft.fftshift(f_transform)\n",
    "magnitude_spectrum = 20 * np.log(np.abs(f_shift) + 1)  # +1 to avoid log(0)\n",
    "\n",
    "# Low-pass filter\n",
    "rows, cols = sample_gray.shape\n",
    "crow, ccol = rows//2, cols//2\n",
    "mask = np.zeros((rows, cols), np.uint8)\n",
    "cv2.circle(mask, (ccol, crow), 50, 1, -1)\n",
    "\n",
    "# Apply mask and inverse transform\n",
    "fshift_filtered = f_shift * mask\n",
    "f_ishift = np.fft.ifftshift(fshift_filtered)\n",
    "img_back = np.fft.ifft2(f_ishift)\n",
    "img_back = np.abs(img_back)\n",
    "\n",
    "show_images([sample_gray, magnitude_spectrum, mask*255, img_back.astype(np.uint8)],\n",
    "           ['Original', 'Magnitude Spectrum', 'Low-pass Mask', 'Filtered Result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e93b60",
   "metadata": {},
   "source": [
    "## 9. Complete Processing Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9dae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_processing_pipeline(img):\n",
    "    \"\"\"Example of a complete image processing pipeline\"\"\"\n",
    "    \n",
    "    # 1. Convert to grayscale if needed\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img.copy()\n",
    "    \n",
    "    # 2. Noise reduction\n",
    "    denoised = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    \n",
    "    # 3. Enhance contrast\n",
    "    enhanced = cv2.equalizeHist(denoised)\n",
    "    \n",
    "    # 4. Edge detection\n",
    "    edges = cv2.Canny(enhanced, 50, 150)\n",
    "    \n",
    "    # 5. Morphological processing to clean up\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    cleaned = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return gray, denoised, enhanced, edges, cleaned\n",
    "\n",
    "# Apply pipeline\n",
    "results = complete_processing_pipeline(sample_img)\n",
    "titles = ['Original', 'Denoised', 'Enhanced', 'Edges', 'Cleaned']\n",
    "\n",
    "show_images(results, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca66cda",
   "metadata": {},
   "source": [
    "## 10. White Balancing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ad83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray_world_white_balance(img):\n",
    "    \"\"\"Gray World white balancing algorithm\"\"\"\n",
    "    avg_b = np.mean(img[:, :, 0])\n",
    "    avg_g = np.mean(img[:, :, 1])\n",
    "    avg_r = np.mean(img[:, :, 2])\n",
    "    \n",
    "    avg = (avg_b + avg_g + avg_r) / 3\n",
    "    \n",
    "    scale_b = avg / avg_b\n",
    "    scale_g = avg / avg_g\n",
    "    scale_r = avg / avg_r\n",
    "    \n",
    "    balanced = np.zeros_like(img, dtype=np.float32)\n",
    "    balanced[:, :, 0] = img[:, :, 0] * scale_b\n",
    "    balanced[:, :, 1] = img[:, :, 1] * scale_g\n",
    "    balanced[:, :, 2] = img[:, :, 2] * scale_r\n",
    "    \n",
    "    return np.clip(balanced, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Apply white balancing\n",
    "balanced_img = gray_world_white_balance(sample_img)\n",
    "\n",
    "show_images([sample_img, balanced_img], \n",
    "           ['Original', 'White Balanced'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055c3ca0",
   "metadata": {},
   "source": [
    "## Quick Debugging Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e21d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_image(img, title=\"Debug Image\"):\n",
    "    \"\"\"Quick function to check image properties and display\"\"\"\n",
    "    print(f\"=== {title} ===\")\n",
    "    print(f\"Shape: {img.shape}\")\n",
    "    print(f\"Type: {img.dtype}\")\n",
    "    print(f\"Min: {img.min()}, Max: {img.max()}\")\n",
    "    print(f\"Mean: {img.mean():.2f}\")\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    if len(img.shape) == 3:\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "debug_image(sample_gray, \"Sample Grayscale Image\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
